%% For double-blind review submission
\documentclass[acmlarge,review]{acmart}\settopmatter{printfolios=true}
%\documentclass[acmlarge,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission
%\documentclass[acmlarge,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission
%\documentclass[acmlarge]{acmart}\settopmatter{}

%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format should change 'acmlarge' to
%% 'sigplan,10pt'.


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{etoolbox}
\usepackage{flushend}
\usepackage{needspace}
\usepackage{algpseudocode}

\makeatletter\if@ACM@journal\makeatother

\makeatletter\preto{\@verbatim}{\topsep=1pt \partopsep=0pt}\makeatother

%% Journal information (used by PACMPL format)
%% Supplied to authors by publisher for camera-ready submission
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{1}
\acmArticle{1}
\acmYear{2017}
\acmMonth{1}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\else\makeatother
%% Conference information (used by SIGPLAN proceedings format)
%% Supplied to authors by publisher for camera-ready submission
\acmConference[PL'17]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2017}{New York, NY, USA}
\acmYear{2017}
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}
\fi


%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission
\setcopyright{none}             %% For review submission
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2017}           %% If different from \acmYear


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations



\begin{document}


%% Title information
\title[SSA GCM]{SSA based global code motion of identical computations}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'

\def \GCC {GCC}
\def \LLVM {LLVM}
\def \SESE {SESE}
\def \CFG {CFG}
\def \SSA {SSA}
\def \MemorySSA {MemorySSA}
\def \PRE {PRE}
\def \GVN {GVN}
\def \LLVMTestSuite {\LLVM{} test-suite}

%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.

\author{Aditya Kumar}
%\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
%\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Senior Compiler Engineer}
  %\department{Department1}              %% \department is recommended
  \institution{Samsung Austin R\&D Center}            %% \institution is required
  %\streetaddress{Street1 Address1}
  \city{Austin}
  \state{Texas}
  %\postcode{Post-Code1}
  \country{USA}
}
\email{aditya.k7@samsung.com}


\author{Sebastian Pop}
%\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
%\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Senior Staff Compiler Engineer}
  %\department{Department1}              %% \department is recommended
  \institution{Samsung Austin R\&D Center}            %% \institution is required
  %\streetaddress{Street1 Address1}
  \city{Austin}
  \state{Texas}
  %\postcode{Post-Code1}
  \country{USA}
}
\email{s.pop@samsung.com}

%% Paper note
%% The \thanks command may be used to create a "paper note" ---
%% similar to a title note or an author note, but not explicitly
%% associated with a particular element.  It will appear immediately
%% above the permission/copyright statement.
\thanks{with paper note}                %% \thanks is optional
                                        %% can be repeated if necesary
                                        %% contents suppressed with 'anonymous'


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  We present a global code motion compiler optimization which schedules
identical computations across the program so as to save code size.  Not only
this code motion saves code size, it exposes redundancies in some cases, it
exposes more instruction level parallelism in the basic block when the
computations are hoisted, and it enables other passes like loop invariant
motion to remove more redundancies. The cost model to drive the code motion is
based on live range analysis on SSA representation such that the (virtual)
register pressure does not increase.

We have implemented the pass in \LLVM{}. It is based on Global Value Numbering
infrastructure available in \LLVM{}. The experimental results show an average
of 2.5\% savings in code size in llvm testsuite, although the code size also
increases in many cases because it enables more inlining. We have also seen
imrpovements of approximately 5\% in a couple of SPEC 2006 benchmarks viz. gcc
and mcf, moreover, register spills reduced for almost all the SPEC 2006
bencmarks when compiled for X86\_64. This is an optimistic algorithm in the
sense that it considers all identical computations in a function as potential
candidates. We make an extra effort to hoist candidates by partitioning the
potential candidates in a way to enable partial hoisting in case common
hoisting points for all the candidates cannot be found. We also formalize how
register pressure will reduce as a result of code-motion and why sorting the
list of potential candidates w.r.t. their depth first numbers helps hoist more
candidates with less compile time overhead.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Compilers}
\ccsdesc[500]{Software and its engineering~Source code generation}
\ccsdesc[500]{General and reference~Performance}

%\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{Optimizng Compilers, GCC, LLVM, Code Generation, Global Scheduling}  %% \keywords is optional

%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}
\label{sec:intro}
Compiler techniques to remove redundant computations are composed of an analysis
phase that detects identical computations in the program and a transformation
phase that reduces the number of run-time computations.  Classical scalar
optimizations like CSE \cite{dragonbook} work very well on single basic blocks but
when it comes to detect redundancies across basic blocks these techniques fall
short: more complex passes like GCSE and \PRE{} have been designed to handle these
cases based on dataflow analysis \cite{morel1979global}.  At first these
techniques were described in the classical data-flow analysis framework, and
later the use of the \SSA{} representation lowered the cost in terms of compilation
time \cite{briggs1994effective,chow1997new,kennedy1999partial} and brought these
techniques in the main stream: nowadays \SSA{} based \PRE{} is available in every
industrial compiler.

This paper describes code-motion of congruent instructions \cite{briggs1997}, a
technique that uses the information computed for \PRE{} to detect identical
computations but has a transformation phase whose goal differs from \PRE{}: it
removes identical computations from different branches of execution.  These
identical computations in different branches of execution are not redundant
computations at run-time and the number of run-time computations is not
reduced. It is not a redundancy elimination pass, and thus it has different cost
function and heuristics than \PRE{} or CSE. It is more similar to global code
scheduling \cite{dragonbook,click1995global} in the sense that it will only move
computations. Code hoisting can reduce the critical path length of execution in
out of order machines. As more instructios are available at the hoisting point,
the hardware has more instructions to reorder. Following example illustrates how
hoisting can improve performance by exposing more ILP.

\begin{verbatim}
float fun(float d, float min, float max, float a) {
  float tmin, tmax, inv;

  inv = 1.0f / d;
  if (inv >= 0) {
    tmin = (min - a) * inv;
    tmax = (max - a) * inv;
  } else {
    tmin = (max - a) * inv;
    tmax = (min - a) * inv;
  }
  return tmax + tmin;
}
\end{verbatim}

In this program the computations of tmax and tmin are identical to the
computations of tmin and tmax of sibling branch respectively. Both tmax and tmin
depends on inv which depends on a division operation which is generally more
expensive than the addition, subtraction and multiplication operations. The
total latency of computation across each branch is:
$C_{div} + 2(C_{sub} + C_{mul})$
Or, for out of order processors with two add units and two multiply units:
$C_{div} + C_{sub} + C_{mul}$

Now if the computation of tmax and tmin are hoisted outside the
conditionals, the C code version would look like this:
\begin{verbatim}
float fun(float d, float min, float max, float a) {
  float tmin, tmax, tmin1, tmax1, inv;

  tmin1 = (min - a);
  tmax1 = (max - a);

  inv = 1.0f / d;
  tmin1 = tmin1 * inv;
  tmax1 = tmax1 * inv;

  if (inv >= 0) {
    tmin = tmin1;
    tmax = tmax1;
  } else {
    tmin = tmax1;
    tmax = tmin1;
  }

  return tmax + tmin;
}

\end{verbatim}

In this code the division operation can be executed in parallel with the two
subtractions because there are no dependencies among them. So the total number
of cycles will be $max(C_{div}, C_{sub}) + C_{mul} = C_{div} + C_{mul}$; since
$C_{div}$ is usually much greater than $C_{sub}$ \cite{x86,aarch64}.

%GCC: 23286 has interesting test cases.

In fact there are several advantages of code-motion of this kind:
\begin{itemize}
\item It helps reduce the code size of the program by replacing multiple
  instructions with one although the final code size may be higher because of
  improved inlining heuristics; functions become cheaper to inline when their
  code size decreases because inliner heuristics in \LLVM{} depends on
  instruction count.
\item It expose more instruction level parallelism to the later compiler
  passes. By hoisting identical computations to be executed earlier, instruction
  schedulers can move heavy computations earlier in order to avoid pipeline
  bubbles;
\item It reduces branch misprediction penalty on out-of-order processors with
  speculative execution of branches: by hoisting/sinking expressions out of
  branches, it can effectively reduce the amount of code to be speculatively
  executed and hence reduce the critical path;
\item It reduces interference/register pressure when appropriate cost-model is
  applied. We have used a cost model in our implementation which hoists/sinks only
  when register pressure would potentially reduce.
\item For SIMD architectures, which execute all branches, it will reduce the
  total number of instructions to be executed.
\item It may improve loop vectorization by reducing a loop with control flow to
  a loop with a single BB, should all the instructions in a conditional get
  hoisted or sinked;
\item It enables more loop invariant code motion (LICM): as LICM passes, in
  general, cannot effectively reason about instructions within conditional
  branches the context of loops, code-hoisting is needed to move instructions
  out of conditional expressions and expose them to LICM.
\end{itemize}

There have been a lot of work both in industry and academia to hoist and sink
code out of branches, and in general global scheduling
\cite{click1995global}. Some relate code-hoisting to code-size optimization
\cite{rosen1988global} and many \cite{barany2013, shobaki2013} use global
scheduling to improve performance. Most of the recent work on global scheduling
are done using ILP which results in prohibitively high compile time. To the best
of our knowledge we have not found any reference which explored code-motion of
identical computation in as much detail as we have done. A part of our
implementation (aggressive code hoisting) is already merged in LLVM trunk,
however, a general implementation of code-motion of congruent (but not
redundant) expressions is still missing from GCC and LLVM trunk. The main
contributions of this paper are:
\begin{itemize}
\item a new optimistic algorithm to move congruent instructiosn out of branches,
\item a cost model to reduce interference and hence, reduce spills,
\item a technique to maximize hoisting in an optimistic approach by partitioning
  the list of potential candidates sorted by their DFS visit number,
\item experimental evaluation of our implementation in \LLVM{} which combines
  SSA based liveness analysis, and ranking of expressions to move very busy
  expressions in order to reduce code-size (and improve performance in some
  cases).
\end{itemize}

\section{Related Work}

There are a lot of bug reports in \GCC{} and \LLVM{} bugzillas
\cite{GCCCodeHoistingBugs, LLVMCodeHoistingBugs}, showing the interest in having
a more powerful code hoist transform.  The current \LLVM{} implementation of
code hoisting in SimplifyCFG.cpp is very limited to hoisting from identical
basic blocks: the instructions of two sibling basic blocks are read at the same
time, and all the instructions of the blocks are hoisted to the common parent
block as long as the compiler is able to prove that the instructions are
equivalent.  This implementation does not allow for an easy extension: first in
terms of compilation time overhead the implementation is quadratic in number of
instructions to bisimulate and second, the equivalence of instructions is
computed by comparing the operands which is neither general nor scalabale.

Dhamdhere \cite{dhamdhere1988fast}, Muchnick \cite{steven1997advanced} mention
code hoisting in a data flow framework. A list of Very Busy Expressions (VBE)
computed which are hoisted in a basic block where the expression is
anticipable (all the operands are available). This algorithm would hoist as far
as possible without regarding the impact on register pressure and as such a cost
model will be required.  Also the description of VBE is based on the classic
dataflow model and an adaptation to a sparse \SSA{} representation is required.

Rosen \cite{rosen1988global} also briefly discuss hoisting computations with
identical value numbers from immediate successors. Their algorithm iterates on
computations of same rank and move the code with identical computations from the
sibling branch to a common dominator if they are very busy
\cite{steven1997advanced}. The cost-model to mitigate register pressure is
missing, also there is no mention of sinking congruent instructions.

\GCC{}ecently got code-hoisting \cite{GCCCodeHoisting} which is implemented as
part of \GVN{}-\PRE{}: it uses the set of ANTIC\_IN and AVAIL\_OUT value
expressions computed for \PRE{}. ANTIC\_IN[B] contains very busy expressions at
basic block B i.e., values computed on all paths from B to exit and
AVAIL\_OUT[B] contains values which are already available. The algorithm hoists
top down to a predecessor.  It uses ANTIC\_IN[B] to know what expressions will
be computed on every path from B to exit, and can be computed in B.  It uses
AVAIL\_OUT[B] to subtract out those values already being computed.  The cost
function is: for each hoist candidate, if all successors of B are dominated by
B, then we know insertion into B will eliminate all the remaining computations.
It then checks to see if at least one successor of B has the value available.
This avoids hoisting it way up the chain to ANTIC\_IN[B].  It also checks to
ensure that B has multiple successors, since hoisting in a straight line is
pointless.  The algorithm continues top down the dominator tree, working in
tandem with \PRE{} until no more hoisting is possible.  One advantage of \GCC{}
implementation is that it works in sync with the \GVN{}-\PRE{} such that when
new hoisting opportunities are created by \GVN{}-\PRE{}, code-hoisting will
hoist them.

Click \cite{click1995global} describe aggressive global code motion to first
schedule all the instructions as early as possible. This results in very long
live ranges which is mitigated by again scheduling all the instructions as late
as possible. They report sppeedup of as high as $23\%$.

Barany \cite{barany2013} presented a global scheduler with integer linear
programming (ILP) formulation with a goal to minimize register pressure. The
results they got were not very promising. It may be because they only used the
scheduler for smaller functions ($<1000$ instructions); also, they compiled the
benchmarks for ARM-Cortex which is more resilient to register pressure because
it has more registers compared to X86, for example.

Shobaki \cite{shobaki2013} also recently presented a combinatorial global
scheduler with reasonable performance improvements. It is possible that both
Barany and Shobaki's implmentation will have similar results when compiled for
same target architecture. Also, both suffer from the same problem, although
Shobaki not so much, of large compile times which is not feasible for industrial
compilers like gcc and LLVM.

We got reduction in register spills on some SPEC2006 benchmarks, reduction in
code-size and some performance improvements with very low compile time overhead.

% TODO: References
% Sink instructions
% Ranking of instructions from Zadeck
% Liveness analysis from Ramakrishna
% Experimental results: Register spills, compile time, run time.

\section{Code motion}

The algorithm for code motion uses several common representations of the
program that we shortly describe below:
\begin{itemize}
\item Dominance (DOM) and Post-Dominance (PDOM) relations \cite{dragonbook} on a
  Control Flow Graph (\CFG{}).
\item DJ-Graph \cite{Sreedhar1996} is a data structure that augments the
  dominator tree with join-edges to keep track of data-flow in a program. We use
  DJ-Graph to compute liveness of variables as illustrated in \cite{das2012}.
\item Static Single Assignment (\SSA{}) \cite{cytron};
\item Global Value Numbering (\GVN{}) \cite{rosen1988global,click1995global}: to
  identify similar computations compilers use \GVN{}.  Each expression is given
  a unique number and the expressions that the compiler can prove to be
  identical are given the same number;
\item Memory \SSA{} \cite{novillo2007memory}: it is a factored use-def chain of
  memory operations that the compiler is able to prove are dependent.
\end{itemize}

The code-motion pass can be broadly divided into the following steps that we will
describe in the rest of this section:
\begin{itemize}
\item find candidates (congruent instructions) suitable for code-motion,
\item compute a point in the program where it is both legal and profitable to
  move the code,
\item move the code to hoist point or sink point, and
\item update data structures to continue iterative code motion.
\end{itemize}

\subsection{Liveness using DJ Graph}
DJ-Graph \cite{Sreedhar1996} is a data structure that augments the dominator
tree with join-edges to keep track of data-flow in a program. We use DJ-Graph
and Merge Sets to compute liveness of variables as illustrated in
\cite{das2012}.  It is very efficient for computing liveness and does not
require any bitvectors to be maintained for each basic block. The underlying
simplicity of liveness computation is due to SSA form, where the the values only
flow (from def to use) either through dominator edges or the join edges (where
we insert a PHI). The DJ-graph contains both these edges which allows for
computation of merge sets for each basic block i.e., a set of all basic blocks
where the values can flow from a particular basic block. We have implemented
merge-set computation from DJ-graph as illustrated in \cite{das2005}. A simplified
version of merge set that we implemented is presented here:

\begin{verbatim}
// Compute merge set top-down in breadth first order.
bool constructMergeSet_1(BFSList B, JEdges JE, DomLevel DL)
repeat = False
// List of visited edges
Visited V
for n in B do
  for e in (all incoming edges to n) do
    if (e is in JE && e not in V) then
      V(e) = true
      Let snode = Source Node of e
      Let tnode = Target Node of e
      Let tmp = snode
      Let lnode = NULL
      while (DL(tmp) ≥ DL(tnode)) do
        Merge(tmp) = Merge(tmp) U Merge(tnode) U {tnode}
        lnode = tmp
        tmp = dom-parent(tmp)
      end while
      repeat = still_inconsistent(lnode, JE, Merge)
    end if
  end for
end while
return repeat

// Construct Merge set of each node in control-flow-graph G of a function.
void  constructMergeSet(CFG G) {
  B = Breadth First Order of G
  JE = JEdges of G
  DL = List of Path length (from root) of each node in G.

  do // Call until a fixed point is reached.
    Repeat = constructMergeSet_1(B, JE, DL);
  while (Repeat);
}
\end{verbatim}

\begin{verbatim}
// Return true if the merge set of source node of
// a visited J-edge (incoming edge of lnode) is not
// the subset of the merge set of lnode.
bool still_inconsistent(Node lnode, JEdges JE, Visited V, MergeSet M)
   for (all incoming edges to lnode) do
     Let e = Incoming edge
     if (e is in JE && e in V) then
       Let snode = Source Node of e
       if (M(snode)!(Subset) M(lnode)) then
         return true
       end if
     end if
   end for
return false
\end{verbatim}


We compute merge sets of the control flow graph which remains same through the
code-motion transformation. For code-motion we only want to know if a use
operand is a kill (to compute changes in register pressure). For that we only
need to know whether the use is also required later in the execution path. A
variable is live out of a basic block B if it is used in the merge set of B. We
compute the live-out relation on-demand when profitability of hoistable/sinkable
candidates is to be evaluated. A simplified version of isLiveOutUsingMergeSet is
presented here:

\begin{verbatim}
bool isLiveOutUsingMergeSet()
  // Compute if variable a is liveout from basic block n
  Input: Node N, Variable a

  if a is defined in N then
    if a is used outside any basic block other than N then
      return true;
    else return false;
  endif

  Ms(n) = null;
  // Mergeset of N is the union of merge sets of its successors
  for w in successors(n) do
    Ms(n) = Ms(n) U Mr(w);
  endfor

  // Iterate over all the uses of a and see if any intersect with the
  // merge set of N.
  for t in users(a) do
    b = basic_block(t)
    while (b != null) and (b != def_bb(a)) do
      if b ∩ Ms(n) then
        return true;
      endif
      b = dom-parent(b);
    endwhile
  endfor
  return false;
\end{verbatim}

\begin{verbatim}
// return true if op is the last use of the
// operand in I.
bool isKill(Operand op, Instruction I) {
  if (isLiveOutUsingMergeSet(op))
    return false;
  B = basic_block(I);
  for each I1 after I in B:
    if (I1 uses op)
      return false;

return true;
}
\end{verbatim}

The original algorithm presented in \cite{das2012} has mismatched types in terms
of uses and nodes because each node can have many instructions and hence many
uses. Also, while iterating on the dominator of each user of a variable we may
reach to the beginning of a function, in that case the inner while loop needs to
terminate. These two cases were missing from the algorithm and we came across
them during implementation. If a variable is not live-out of a basic block, it
still may be used later in the basic block. To establish whether a use of a
variable is a kill we iterate on all the subsequent instructions in a basic
block checking for uses should isLiveOutUsingMergeSet return false.

\subsection{Finding candidates to move}
\label{subsec:finding-candidates}
The first step is to find a set of congruent instructions
\cite{briggs1997}. This is performed by a linear scan of all instructions of the
program and classifying them by their value numbers. We could compute available
and anticipable sets as computed by GCC's code-hoisting but that would be a lot
of data structures to maintain at each basic block level. For GCC it makes sense
because their code-hoisting is integrated with GVN-PRE which already has those
data structures available.

The current implementation of \GVN{} in \LLVM{} has some limitations when it
comes to loads and stores so we compute the \GVN{} of loads and stores
separately.  Our solution to value number loads is to hash the address from
where the value is to be loaded. For stores, we value number the address as well
as the value to be stored at that address. Another limitation of the current
\GVN{} implementation in \LLVM{} is that the instructions dependent on the loads
will not get numbered correctly, and so after hoisting all candidates we need to
rerun the \GVN{} analysis in order to discover new candidates now available
after having hoisted load instructions.  This limitation should be addressed in
a new implementation of the \GVN{} based on \MemorySSA{}, that would better
account for equivalent loads and their dependent instructions.

The process of computing \GVN{} can be on-demand (as we come across an
instruction) or, precomputed (computing \GVN{} of all the instructions
beforehand). Which process to choose is determined by the scope of code-hoisting
we want to perform. In a pessimistic approach, we want to hoist a limited set of
instructions from the sibling branches as we iterate the DFS tree bottom-up, it
is sufficient to compute \GVN{} values on-demand. Whereas, in the optimistic
approach, as described in Section~\ref{subsec:optimistic}, we want to move as
many instructions as possible, and it would require \GVN{} values to be
precomputed.

Once the instructions have been classified into congruence classes, we compute
for each group of congruent instructions, a point in the program that is both
legal and profitable for the instructions to be moved to.

\subsection{Legality check}
\label{subsec:legality}
Since the equality of candidates is purely based on the value numbers, we also
need to establish if hoisting them to a common dominator or sinking them to a
common post-dominator would be legal. Once a common dominator (post-dominator)
is found, we check whether all the use-operands of the set of instructions are
available at that position. It is possible reinstantiate (remateralize) the
use-operands in some cases when the operands are not available and make it legal
to move the instruction.

Subsequently, it is checked that the side-effects of the computations (if any)
do not intersect with any side-effects between the instructions to be
hoisted/sunk and their hoisting/sinking point. It is also necessary to check if
there are indirect branch targets e.g., landing pad, case statements, goto
labels etc., along the path because it becomes difficult to prove safety checks
in those cases. In our current implementation we discard candidates on those
paths.

\subsubsection{Legality of hoisting scalars}
Scalars are the easiest to hoist because we do not have to analyze them for
aliasing memory references. As long as all the operands are available (or can be
made available by rematerialization), the scalar computations can be hoisted.

\subsubsection{Legality of hoisting loads}
The availability of operand to the load (an address) is checked at the hoisting
point. If that is not available we try to rematerialize the addresss if
possible.  Along the path, from current position of the load instruction
backwards on the control flow to the hoisting point, we check whether there are
writes to memory that may alias with the load, in which case the candidate is
discarded. To iterate on the use-def chains for memory references the MemorySSA
infrastructure of \LLVM{} is used.

\subsubsection{Legality of hoisting stores}
For stores, we check the dependency requirements similar to the hoisting of
loads using the MemorySSA of \LLVM{}. We check that the operands of the store
instruction are available at the hoisting point, and that there are no aliasing
loads or store along the path from the current position to the hoisting point.

\subsubsection{Legality of hoisting calls}
Call instructions can be divided into three categories: those calls equivalent
to purely scalar computations, calls reading from memory, and most of the time,
without further information, calls have to be classified as writing to memory,
that is the most restrictive form.  Each category of call instructions is
handled as described for scalar, load, and store instructions.

Hoisting loads/stores across calls also require precise analysis of all the
memory addresses accessed by the call. The current implementation being an
intraprocedural pass, cannot hoist aggressively across calls. In the presence of
pure calls, loads can be hoisted but stores can't. Also, if a call throws
exceptions, or if it may not return, nothing can be hoisted across that call.

\subsubsection{Legality of sinking expressions}
Sometimes, hoisting is not upward-safe \ref{click1995global} e.g., if the
expressions are in a landing pad etc., in that case sinking of those expressions
may reduce code size. Sinking may also reduce the register pressure in some
cases e.g., when the use operands are not kills.For sinking, higher ranked
expressions would be sunk first. And it would be illegal to sink higher ranked
identical expressions if they are not anticipable in the common
post-dominator. For example:

\begin{verbatim}
B0: i0 = load B
B1: i1 = load A
    c1 = i1 + 10
    d1 = i0 + 20
    goto B3
B2: i2 = load A
    c2 = i2 + 10
    d2 = i0 + 20
    goto B3

B3: phi(c1, c2)
    phi(d1, d2)
\end{verbatim}

In this example (c1, c2) or (d1, d2) are potential sinkable candidates. Since
(c1, c2) depend on i1 and i2 respectively which are also in their original basic
blocks, c1 and c2 are not anticipable in B3. So without knowing the sinkability
of `i1' and `i2' it would be illegal to sink (c1, c2) to B3. On the other hand
(d1, d2) can safely be sunk because their operands are readily available at the
sink point i.e., B3. It should also be noted that, just because the expressions
are identical and operands are available, it still requires a unique
post-dominating PHI to use the exact same values to be legally sinkable.

A general global scheduling algorithm also requires checks for undefinedness
when introducing a new computation along a path. Since only very busy
expressions are moved in the current implementation, there is no need to check
for undefinedness resulting due to movement of instructions. This simplifies the
implementation.

\subsection{Profitability check (Cost models)}
\label{sec:cost-models}
After the legality checks have passed, we check if a code-motion is profitable.
That takes into account the impact code-motion would have on various parameters
that might affect runtime performance e.g., impact on live-range, gain in the
code size. Since this is mostly a code-size optimization pass, the goal is to
not regress in performance across popular benchmarks at the same time reduce
code size as much as possible. Following are the cost models which are
implemented:

\subsubsection{Reduce register pressure}
\label{hoist:reg-pressure}
Hoisting upwards will decrease the live-range of its use, if it is a last use (a
kill), but increase the live-range of its definition. Conversely, sinking will
decrease the live-range of the defined register but increase the live-range for
killed operands. If the live-range after code-motion is less than before it will
be moved. Essentially, as long as there is one killed operand , code hoisting
will either decrease or preserve the register pressure.  Similarly, code-sinking
will either decrease or preserve the register pressure as long as there is one
operand killed at most.  Following example explains how code motion of identical
computations can reduce the register pressure.  Consider the following example
where the labels prefixed with 'P' represent the position of instruction in a
basic block (names prefixed with 'B').

\begin{verbatim}
    b = m
    c = n
B0: if c is true then goto B3 else goto B4

B1: a0 = b<kill> + c<kill>

B2: a1 = b<kill> + c<kill>

After hoisting a0 and a1 are removed and a copy
of a0 as a01 is placed in B0 just before the branch.
\end{verbatim}

In this case, since `b' and `c' are killed in `a0' and `a1', hoisting them will
reduce the register pressure in B3 and B4 because two registers will be freed.

Ideally, it should be okay to hoist all the instructions and a later a
live-range-splitting \cite{cooper1998live} pass should make the right decision
of rematerializing the instruction should it be beneficial to do so. But the
current live-range splitting pass of \LLVM{} is not making the optimial decision
and we have found regressions while hoisting aggressively.

Moreover, LLVM has a `getelementptr' instruction which computes the address
where a load or a store would happen. It is a scalar computation and gets
hoisted frequently even if the corresponding loads/stores would not get hoisted.
In order to reduce register pressure while hoisting loads, we have restricted
hoisting of address computations away from their corresponding loads and stores
when the loads and stores cannot be moved.  This restriction is only to mitigate
the limitations of LLVM's register allocator and may be lifted in the future,
when the register allocation rematerialization pass has been improved to catch
these regressions.

\subsubsection{Hoisting an expression across a call}
\label{cost:across-calls}
Even hoisting scalars across calls is tricky because it can increase the number
of spills. During the frame lowering of calls, the argument registers also known
as the caller saved registers are saved because they might be modified by the
callee and after the call they are restored. So before the call, the register
pressure is high because the number of available registers are reduced by the
number of caller saved registers. In that situation a computation that increases
register pressure is not profitable to hoist.

\subsubsection{Partitioning the list of hoistable candidates to maximize hoisting}
\label{subsec:partition}
%The partition algorithm is described as follows:
In the approach, described in Section~\ref{subsec:optimistic}, it is possible
that a common hoisting point for all the instructions does not satisfy legality
or profitability checks. In these cases, it is still possible to `partially'
hoist a subset of instructions by splitting the set of candidates and finding a
closer hoisting point for each subset.

In order to hoist a subset of identical instructions, we partition the list of
all candidates in a way to maximize the total number of hoistings.  By sorting
the list of all the candidates in the increasing order of their depth first
search discovery time stamp \cite{clrs} (DFSIn numbers), we make sure that
candidates closer in the list have their common dominator nearby. In
Figure-\ref{fig:dfsin}, if B3, B4, B5, and B6 have identical computations and if
for some reason, they cannot be hoisted at B0, then we partition the set of
hoistable candidates in their DFSIn order. In this case the DFSIn ordering would
be B3, B4, B5, B6 which will allow the instructions in B3, B4 to be hoisted at
B1 and those in B5, B6 to be hoisted at B2. Even if (B3, B4) could not be
hoisted, for some reason, to B1, (B5, B6) can still be hoisted -- if legal and
profitable -- to B2.

\begin{figure}
\centering
%rankdir=LR;
\digraph[scale=0.35]{abc}{
  B0 -> B1
  B0 -> B2
  B1 -> B3
  B1 -> B4
  B2 -> B5
  B2 -> B6
}
\caption{CFG to illustrate partitioning}
\label{fig:dfsin}
\end{figure}

% TODO: Rewrite the following.
In our current implementation we keep as many candidates in one set as possible
(greedy approach). We split the list at a point where the legality checks fail
to hoist subset of candidates which are legal to hoist and then start finding
new hoisting point for the remaining ones.


There is a special case for hoisting load instructions when the hoist-point is
the predecessor basic block for all the loads. Even if the register pressure
would increase, we prefer to hoist loads in this case. The reason being, that
would make the loaded value available by the time it is used. Also, because
stores and calls are hoisted the least \ref{table:experimental-results}, the
performance does not change much whether they are hoisted or not.

\subsection{Code generation}
Once all the legality and profitability checks are satisfied for a set of
congruent instructions, they are suitable candidates for hoisting. A copy of the
computation is inserted at the hoisting point along with any instructions which
needed to be rematerialized. Thereafter, all the computations made redundant by
the new copy are removed, and the \SSA{} form is restored by updating the
intermediate representation (IR) to reflect the changes. At the same time
MemorySSA is also updated to get up-to-date information about memory references.

After one iteration of algorithm runs through the entire function, it creates
more oppportunities for \emph{higher ranked} computations
\cite{rosen1988global}. Currently, this is a limitation of the \GVN{} analysis
pass, and so we rerun the code-hoisting algorithm until there are no more
instructions left to be hoisted.  Obviously, this is not the most optimal
approach and can be improved by ranking the computations \cite{rosen1988global},
or by improving the \GVN{} analysis to correctly populate congruence classes.

Finally after the transformation is done, we verify a set of post-conditions to
establish that program invariants are maintained: e.g., consistency of use-defs,
and \SSA{} semantics.

\section{SSA based global code motion of identical computations}
\label{sec:implementation-details}
% Move this before legality and profitability??
% Ranking of instructions from Zadeck
% Liveness analysis from Ramakrishna
In this section we will present an overview of the algorithm we implemented in
\LLVM{}.  The amount of hoisting depends on whether we collect \GVN{} of
instructions before finding candidates (optimistic) or, on-demand
(pessimistic). It also depends on the generality of the \GVN{} algorithm as
mentioned earlier in Section~\ref{subsec:finding-candidates}. We have
implemented a optimistic global code motion of congruent instructions which uses
the liveness analysis as illustrated in Das \cite{das2012} and ranking
expressions explained in Rosen \cite{rosen1988global}.

Code-motion basically consists of two parts i.e., hoisting and sinking. This
implementation only moves congruent instructions. An immediate guarantee of this
approach is gain in code size (the final executable may be of larger size
because of more inlining). The algorithm prefers hoisting to sinking. If the
dependency of a hoistable candidate is in the same basic block as the candidate,
then the dependency must also be hoistable otherwise hoisting will be illegal or
would require a complicated code generation to make it legal. The current
algorithm discards cases if the dependency is neither hoistable nor
rematerializable.

\subsection{Optimistic code motion algorithm}
\label{subsec:optimistic}
We collect the \GVN{} of all the instructions in the function and iterate on the
list of instructions having identical \GVN{}s. The algorithm prefers hoisting to
sinking. So first we find the common dominator dominating all such identical
computations and perform legality checks, as described in
Section~\ref{subsec:legality}. Often times it is not possible to hoist all the
instructions to one common dominator, due to legality constraints, e.g.,
intersecting side-effects, or profitability constraints, e.g., increasing
register pressure; in those cases, this algorithmn would partition
(Section~\ref{subsec:partition}) the list of identical instructions into subsets
which can be partially hoisted to their respective common dominators. However,
it should be noted that we only hoist very busy expressions to avoid checking
for undefined behaviors resulting because of introducing extra computation in an
execution path.

Barriers are based on the concept of pinned instructions \cite{click1995global}
but extended to adapt to LLVM IR. Since a basic block in LLVM IR is actually an
extended basic block because there might be calls in the middle of a basic block
which might not return. So barriers can be present in the middle of a basic
block. Essentially, any instruction that cannot not guarantee progress is marked
as a barrier. In the absense of context (as in our current implementation), some
instructions which might be safe are still classified a barrier e.g., volatile
loads/stores, calls with missing attributes.

\begin{verbatim}
computing barriers:
for each basic block B in a function:
  barrier_found = false
  for each instruction I in B:
    if I does not guarantee progress:
      mark I as a barrier instruction
      barrier_found = true
      break;

  // Find the last barrier below which instructions can be sunk.
  // If there was no barrier in B, any instruction satisfying
  // other legality checks can be sunk.
  if barrier_found:
    for each instruction I in B in reverse order:
      if I does not guarantee progress:
        mark I as a sink barrier
        break
\end{verbatim}

\begin{verbatim}
computing downward-safety of hoistable instructions at hoist-point
Worklist = list of all basic blocks of hoistable instructions
for each basic block B in the dominator tree starting at hoist-point:
  if Worklist is empty
    return false // Path exists! not downward safe
  if B is in Worklist:
    remove B from worklist
    remove subtree with root at B // Available at B => downward safe from B
  if B is a leaf basic block:
    return false  // Path exists! not downward safe
  if B dominates hoist-point:
    return false  // Back edge

\end{verbatim}

% Explain the overall algorithm, explain how anticipability is computed on demand
% explain the pinned-instructions/barriers.
\begin{verbatim}
Analyses available: Dominator Tree, DFS Numbering, Memory SSA,

Compute GVN of each expression in the function
Compute DJ Graph of function and compute mergeset based on that

For each value number VN with 2 or more instructions:
  sort the instructions according to DFSIn numbers
  if first two I1, I2 are hoistable:
    if they are downward-safe at hoist-point:
      if they are profitable to be hoisted at hoist-point:
        proceed to check hoistability of subsequent instruction with same VN
      else
        proceed to check if I2 and I3 (if available) is hoistable

For each hoistable VN:
  move the first one I1 to the hoist-point
  update the use of all other candidates to refer to I1
  remove all others.
  if I1 has memory references (a load, store etc.)
    update MemorySSA of I1 and others to point to the MemorySSA reference of I1
    remove MemorySSA reference of all others which were deleted
  update statistics

If any of candidates were hoisted then repeat hoisting
Once no more candidates are hoistable proceed to sinking
\end{verbatim}


\begin{verbatim}
Analyses available: Dominator Tree, DFS Numbering, Memory SSA, Mergeset
Compute GVN of each expression in the function

For each value number VN with 2 or more instructions:
  if there are two instructions with same immediate successor as post-dominator
    if both are only used in the same PHI-node of the successor
      if both have dependencies that are available or can be made available at the sink point
         mark VN as sinkable

For each pair I1, I2 of sinkable instructions:
  Move the first one I1 to the sink point which is just after all the PHI-nodes
  in the post-dominator
  update the use of PHI with I1
  Remove I2 and PHI
  if I1 is a memory reference:
    update MemorySSA of I1 and others to point to the MemorySSA reference of I1
    remove MemorySSA reference of all others which were deleted
  update statistics

\end{verbatim}

Code hoisting opens new opportunities for other hoistable candidates which were
of higher rank (depended on candidates which got hoisted). Ideally we could
iterate on lower ranking expressions first and then proceed to higher ranking
expressions in the same iteration but LLVM's GVN infrastucture does not compute
equivalence classes in a effective way. We found it simpler to just recompute
the value numbers and start finding hoistable candidates again.

\subsection{Time complexity of algorithm}
The complexity of code hoisting is linear in number of instructions that are
candidates for code-motion, matching the complexity of \PRE{} on \SSA{} form.
The analyses computed for this pass are Global Value Numbering, Computation of
DJ Graph and MergeSets, Marking Barriers, all are linear in number of
instructions in a function. Liveness analysis expensive but only performed
on-demand for hoistable candidates so it does not affect the overall compile
time by much. Other analyses like Alias Analysis, Memory SSA and Dominator Tree
are already available in the LLVM pass pipeline. Although we recompute GVN, and
Barriers for each iteration of the code-motion, we have not seen signinficant
increase in the compilation times. We have also provided appropriate compiler
flags to expedite code-motion by bailing out with fewer iterations, or skip the
liveness based profitability analysis to aggressively move the code as long as
they are legal (Section~\ref{sec:experimental-results}).

The analysis is followed by a simple code generation that adds the identified
instruction in the destination point and removes all instructions rendered
redundant as a result of code-motion.


\section{Experimental Evaluation}
\label{sec:experimental-results}
% Experimental results: Register spills, compile time, run time. compile-time comparison w.r.t. gcc
We ran \LLVMTestSuite{} (trunk:d87471f8) with the patch (trunk:86940146). All
the experiments were conducted on x86\_64 Ubuntu-Linux machine and at -Ofast
optimization level.  The results for code-hoisting are listed in
Table~\ref{tab:hoist-results}. The table lists the number of scalars, loads,
stores and calls hoisted as well as removed. For each category, the number of
instructions removed is greater or equal to the number of instructions hoisted
because each hoisting is performed only when at least one identical computation
is found.

Loads are hoisted the most followed by scalars, stores and calls in decreasing
order.  This was the common trend in all our experiments. One reason why loads
are hoisted the most is the early execution of this pass (before mem2reg) in the
\LLVM{} pass pipeline. Passes like mem2reg, instcombine might actually remove
those loads so this order may change should this pass be scheduled later.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|}
      \hline
      Metric               & Number\\\hline
      Scalars hoisted      & 6791  \\\hline
      Scalars removed      & 9696  \\\hline
      Loads hoisted        & 14802 \\\hline
      Loads removed        & 20719 \\\hline
      Stores hoisted       & 15    \\\hline
      Stores removed       & 15    \\\hline
      Calls hoisted        & 8     \\\hline
      Calls removed        & 8     \\\hline
      Total Instructions hoisted & 21616 \\\hline
      Total Instructions removed & 30438 \\\hline
\end{tabular}
  \end{center}
  \caption{Code hoisting metrics on \LLVMTestSuite{}}
  \label{tab:hoist-results}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|c|}
      \hline
      Metric               & Before & After              \\\hline
      Functions deleted (all callers found)       & 38250   & 38255  \\\hline
      Functions inlined                           & 154986  & 154985 \\\hline
      Spills inserted                             & 50724   & 50784  \\\hline
      LICM
\end{tabular}
  \end{center}
  \caption{Static metrics before and after code-hoisting on \LLVMTestSuite{}}
  \label{tab:static-results}
\end{table}


Other static metrics are listed in Table~\ref{tab:static-results}. Here we can
see that except for rematerializing defs for splitting, which has an overhead of ~2\%, all
other parameters have less than 1\% overhead. This is to explain why the performance does not
go down with our implementation (and cost-model) of code hoisting pass.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|}
      \hline
      Code-size metric  (.text)                   & Number   \\\hline
      Total  benchmarks                           & 497      \\\hline
      Total  gained in size                       & 39       \\\hline
      Total  decrease in size                     & 58       \\\hline
      Median decrease in size                     & 2.9\%    \\\hline
      Median increase in size                     & 2.4\%    \\\hline
    \end{tabular}
  \end{center}
  \caption{Code size metrics on \LLVMTestSuite{}}
  \label{tab:code-size}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
      \hline
Number of spills  &base(-Ofast) &code-motion(-Ofast) &\%loss \\\hline
400.perlbench	  & 2539	& 2493	& 0.98
401.bzip2	  & 718	        & 707	& 0.98
403.gcc	          & 5782	& 5722	& 0.99
429.mcf	          & 14	        & 17	& 1.21
433.milc	  & 635	        & 642	& 1.01
444.namd	  & 3223	& 3274	& 1.01
445.gobmk	  & 2166	& 2173	& 1.00
447.dealII	  & 11074	& 10234	& 0.92
450.soplex	  & 1122	& 1125	& 1.00
453.povray	  & 4701	& 4692	& 1.09
456.hmmer	  & 1197	& 1274	& 1.06
458.sjeng	  & 168	        & 176	& 1.05
462.libquantum	  & 118	        & 118	& 1
464.h264ref	  & 3379	& 3454	& 1.02
470.lbm	          & 33	        & 33	& 1
471.omnetpp	  & 524	        & 527	& 1.00
473.astar	  & 180	        & 201	& 1.12
482.sphinx3	  & 674	        & 670	& 0.99
483.xalancbmk	  & 5102	& 4965	& 0.97
Grand Total	  & 43349	& 42497	& 0.98
    \end{tabular}
  \end{center}
  \caption{Number of spills on SPEC2006 at -Ofast}
  \label{tab:code-size}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
      \hline

      TODO: Check if putting code-size or spills number is okay or only % is required.
Code size         & base(ofast)	& ofast+gvnhoist&	increase in codesize \\\hline
400.perlbench	  &  2469594	& 2478714	& 1.00 \\\hline
401.bzip2	  &  865902	& 867502	& 1.00 \\\hline
403.gcc	          &  4379291	& 4437467	& 1.01 \\\hline
429.mcf	          &  794909	& 794749	& 1.00 \\\hline
433.milc	  &  1009553	& 1010209	& 1.00 \\\hline
444.namd	  &  282794	& 286658	& 1.01 \\\hline
445.gobmk	  &  2474193	& 2494673	& 1.01 \\\hline
447.dealII	  &  3577836	& 3605672	& 1.01 \\\hline
450.soplex	  &  411254	& 409034	& 0.99 \\\hline
453.povray	  &  1114405	& 1114241	& 1.00 \\\hline
456.hmmer	  &  1421823	& 1425287	& 1.00 \\\hline
458.sjeng	  &  941066	& 940626	& 1.00 \\\hline
462.libquantum	  &  911028	& 911124	& 1.00 \\\hline
464.h264ref	  &  1677417	& 1680841	& 1.00 \\\hline
470.lbm	          &  795444	& 795444	& 1    \\\hline
471.omnetpp	  &  661099	& 658411	& 0.99 \\\hline
473.astar	  &  44296	& 45224		& 1.02 \\\hline
482.sphinx3	  &  1337445	& 1334437	& 1.00 \\\hline
483.xalancbmk	  &  4293600	& 4247108	& 0.99 \\\hline
    \end{tabular}
  \end{center}
  \caption{Number of spills on SPEC2006 at -Ofast}
  \label{tab:code-size}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
      \hline
 hoist licm	sunk licm		functions inlined		f deleted	
 base	& gvn	& base	& gvn	& base	& gvn	& base	& gvn
 400.perlbench	  & 2428	& 2480	& 36	& 35	& 1562	& 1565	& 232	& 234
 401.bzip2	  & 721	& 825	& 3	& 3	& 172	& 180	& 32	& 33
 403.gcc	          & 8867	& 10079	& 143	& 182	& 5034	& 5080	& 1303	& 1304
 429.mcf	          & 55	& 51	& 1	& 1	& 4	& 4	& 	& 
 433.milc	  & 2537	& 2795	& 3	& 3	& 75	& 81	& 	& 
 444.namd	  & 2204	& 2206	& 240	& 240	& 387	& 390	& 57	& 55
 445.gobmk	  & 3090	& 3107	& 18	& 19	& 735	& 775	& 230	& 234
 447.dealII	  & 59114	& 95884	& 60	& 8	& 100891	& 100692	& 30219	& 29966
 450.soplex	  & 4668	& 4848	& 18	& 17	& 9801	& 9784	& 3125	& 3112
 453.povray	  & 9533	& 9079	& 23	& 32	& 5522	& 5532	& 1202	& 1201
 456.hmmer	  & 3200	& 3232	& 3	& 8	& 562	& 562	& 103	& 101
 458.sjeng	  & 400	& 446	& 	& 	& 127	& 135	& 18	& 18
 462.libquantum	  & 362	& 386	& 	& 	& 87	& 87	& 21	& 21
 464.h264ref	  & 17075	& 17153	& 4	& 4	& 338	& 382	& 72	& 72
 470.lbm	          & 39	& 39	& 	& 	& 13	& 13	& 4	& 4
 471.omnetpp	  & 565	& 661	& 32	& 30	& 4790	& 4843	& 1321	& 1316
 473.astar	  & 691	& 754	& 	& 	& 359	& 363	& 113	& 113
 482.sphinx3	  & 1599	& 1693	& 11	& 9	& 198	& 194	& 53	& 53
 483.xalancbmk	  & 9997	& 11453	& 56	& 54	& 69534	& 68839	& 29875	& 29519

    \end{tabular}
  \end{center}
  \caption{Number of spills on SPEC2006 at -Ofast}
  \label{tab:code-size}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|c|c|}
      \hline
 code-motion metric &  I hoisted	I removed	loads hoisted	loads removed	calls hoisted	I sunk	stores hoisted	stores removed
 400.perlbench	  &  2387	& 2456	& 1493	& 1560	& 1	& 4	& 38	& 38
 401.bzip2	  &  176	& 300	& 149	& 273	& 	& 	& 	& 
 403.gcc	          &  5670	& 5761	& 3109	& 3191	& 	& 1	& 3	& 3
 429.mcf	          &  42	& 43	& 37	& 38	& 	& 	& 	& 
 433.milc	  &  128	& 130	& 117	& 119	& 1	& 	& 	& 
 444.namd	  &  262	& 264	& 148	& 150	& 1	& 	& 	& 
 445.gobmk	  &  1021	& 1052	& 705	& 734	& 1	& 1	& 3	& 3
 447.dealII	  &  10455	& 12325	& 6394	& 8210	& 32	& 	& 20	& 20
 450.soplex	  &  2389	& 2731	& 1437	& 1702	& 11	& 	& 14	& 14
 453.povray	  &  1967	& 2269	& 1389	& 1661	& 14	& 	& 4	& 4
 456.hmmer	  &  408	& 427	& 319	& 338	& 6	& 	& 1	& 1
 458.sjeng	  &  372	& 399	& 234	& 251	& 9	& 1	& 1	& 1
 462.libquantum	  &  49	& 49	& 32	& 32	& 	& 	& 	& 
 464.h264ref	  &  2953	& 3438	& 2238	& 2656	& 1	& 1	& 2	& 2
 470.lbm	          &  23	& 23	& 19	& 19	& 	& 	& 	& 
 471.omnetpp	  &  391	& 408	& 297	& 314	& 2	& 	& 	& 
 473.astar	  &  202	& 222	& 140	& 158	& 	& 	& 	& 
 482.sphinx3	  &  168	& 172	& 136	& 140	& 	& 	& 	& 
 483.xalancbmk	  &  5894	& 6162	& 3301	& 3495	& 3	& 2	& 4	& 4
        &         &         &         &         &         &         & 
    \end{tabular}
  \end{center}
  \caption{Number of spills on SPEC2006 at -Ofast}
  \label{tab:code-size}
\end{table}


While benchmarking \LLVMTestSuite{} we see both increase as well as decrease in
the codesizes of the final binaries. Since the pass runs early, it affects many
optimizations which rely on the number of instructions, lenth of the use-def
chain, and other metrics. For instance, the inliner is impacted by a decrease in
the number of instructions in the caller and callee, as its heuristics estimate
the size of functions to be inlined. Various code-size metrics are shown in
Table~\ref{tab:code-size}. All but one benchmark varied between -5.32\% and
5.43\%.  In one benchmark FreeBench/distray/distray.test, the codesize increased
by 35.38\%. In this benchmark 3 more functions got inlined (15 as compared to
12) and because of that 10 more vector instructions got generated (81 vs. 71), 3
calls got hoisted/sunk as (compared to 0), one loop got unswitched (compared to
0), 6 high latency machine instructions got hoisted out of loop, 59 (compared to
30) machine instructions got hoisted out of loop, 70 (compared to 39) machine
instructions were sunk.

The code shown in Section~\ref{sec:intro} is a reduced example that appears in a
hot loop of a proprietary benchmark.  When the expressions are hoisted from the
conditional clauses, the overall performance of that benchmark improves by
$15\%$ on an out-of-order processor due to increased instruction level
parallelism, and better scheduling of the instructions, accommodating for the
long latency of the division operation.

\section{Conclusion and Future Work}
\label{sec:future-work}
We have presented the \GVN{} based code hoisting algorithm. The primary goal is to
reduce the code size but it benefits performance in some cases as well. To
preserve performance and not hoist too much we have implemented several cost
models described in Section~\ref{sec:cost-models}. Since those cost models
depend on a set of thresholds, it requires tuning, as such, we used
representative benchmarks to tune them.

In general it is a good idea to start with lower ranked expression first such
that maximum hoisting can happen in one iteration, however, current
implementation does not rank the expressions and iteratively finds a fixed point
when no more candidates are available. Even this implementation converges
quickly and no significant compile time regression have been observed because of
code hoisting pass. This is not the most optimal approach and results in
multiple data structures to be recomputed. This can be improved by ranking the
computations \cite{rosen1988global}. Also \GVN{}-hoist runs very early in the
pass pipeline, it will be good to evaluate the codesize/performance impact when
it is run in sync with \GVN{}-\PRE{} just like \GCC{} does.

With the implementation of code-hoisting in \LLVM{}, the passes which rely on the
code-size/instruction-count to make optimization decisions needs to be
revisited. The first candidate would be the inliner. We have seen different
inlining decisions in Table~\ref{tab:code-size}, before and after
code-hoisting was enabled.  Since inliner has several magic numbers tuned for
the previous pass layout, it would need some improvement.

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
We would like to thank Daniel Berlin for his code reviews and for his feedback
on earlier versions of this paper and Brian Grayson for motivating examples that
started this work.
\end{acks}


%% Bibliography
\bibliography{Bibliography}

%% Appendix
%\appendix
%\section{Appendix}
%Text of appendix \ldots

\end{document}
