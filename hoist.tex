\documentclass{sig-alternate}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{etoolbox}
\usepackage{flushend}
\usepackage{needspace}

\makeatletter
\preto{\@verbatim}{\topsep=1pt \partopsep=0pt}
\makeatother

\pagenumbering{arabic}

\begin{document}
\def \SCoP {SCoP}
\def \GCC {GCC}
\def \LLVM {LLVM}
\def \SESE {SESE}
\def \CFG {CFG}
\def \SSA {SSA}
\def \scev {scev}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\title{GVN-Hoist: Hoisting Computations from Branches}

\toappear{
   \hrule \vspace{5pt}
   LLVM-dev 2016
}
\numberofauthors{2}

\author{
\alignauthor
Aditya Kumar\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{aditya.k7@samsung.com}
\alignauthor
Sebastian Pop\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{s.pop@samsung.com}
}

\maketitle
\begin{abstract}
  Code hoisting identifies identical computations across the program with the
  help of a Global Value Numbering (GVN) analysis, identifies a hoisting
  position that dominates all the identical computations and in which all
  operands are avaialable, duplicates one of the expressions in the hoisting
  position, and removes all the computations now redundant in their original
  position.

  Code hoisting is often confused with CSE (Common Subexpression Elimination)
  although hoisting is not bound to find redundancies within a single basic
  block.  Code hoisting could also be confused with Partial Redundancy
  Elimination (PRE) or even with Global Common Subexpression Elimination (GCSE)
  although the main goal of code hoisting is not to remove redundancies: code
  hoisting effectively exposes redundancies and enables other passes like LICM
  to remove more redundancies.  The main goal of code hoisting is to reduce code
  size with the added benefit of exposing more instruction level parallelism and
  reduced register pressure.
\end{abstract}

\section{Introduction}

Compiler techniques to remove redundant computations are composed of an analysis
phase that detects identical computations in the program and a transformation
phase that reduces the number of run-time computations.  Classical scalar
optimizations like CSE \cite{dragonbook} work very well on single basic blocks.
When it comes to detect redundancies across basic blocks these techniques fall
short: more complex passes like GCSE and PRE have been designed to handle these
cases based on dataflow analysis \cite{morel1979global}.  At first these
techniques were described in the classical data-flow analysis framework, and
later the use of the SSA representation lowered the cost in terms of compilation
time \cite{briggs1994effective,chow1997new,kennedy1999partial} and brought these
techniques in the main stream: nowadays SSA based PRE is available in every
industrial compiler.

This paper describes code hoisting, a technique that uses the information
computed for PRE to detect identical computations and has a transformation phase
whose goal differs from PRE: it removes identical computations from different
branches of execution.  These identical computations would not be redundant at
run-time and thus the number of run-time computations is not reduced.  Instead,
the goals of code hoisting are:
\begin{itemize}
\item to reduce the code size of the program;
\item to remove fully redundant instructions: although this is also done by
  GVN-PRE.
\item to improve function inlining heuristics: functions become cheaper to
  inline by reducing their code size;
\item to expose more instruction level parallelism: by hoisting identical
  computations to be executed earlier, schedulers can move heavy computations
  earlier in order to avoid pipeline bubbles;
\item to help out-of-order processors with speculative execution of branches: by
  hoisting expressions out of branches, code hoisting can effectively reduce the
  amount of code to be speculatively executed and can reduce the critical path;
\item to reduce register pressure: by moving computations closer to the
  definitions of their operands;
\item to improve passes that do not work well with branches:
  \begin{itemize}
  \item to improve loop vectorization by reducing a loop with control flow to a
    loop with a single BB, should all the instructions in a conditional get
    hoisted and sinked;
  \item to enable more loop invariant code motion (LICM): as LICM does not
    reason about instructions in the context of loops with conditional branches,
    code-hoisting is needed to move instructions out of conditional expressions
    and expose them to LICM.
  \end{itemize}
\end{itemize}

The complexity of code hoisting is linear in number of instructions that could
be hoisted in the program, matching the complexity of PRE on SSA form.  The
analysis phase is based on the Global Value Numbering (GVN), the same analysis
used for PRE, followed by the computation of a partition of identical
expressions to be hoisted in a same location to guarantee safety properties and
program performance, and followed by a simple code generation that adds the
identified instruction in the hoisting point and removes all the now redundant
expressions.

\subsection{Contributions of this paper}
\begin{itemize}
\item A systematic approach to writing a compiler optimization pass.
\item gvn hoisting
\item partition algorithm
\item cost model to reduce live-range and reduce spills
\end{itemize}


\begin{itemize}
\item Code hoisting in adjacent blocks in a diamond/triangle like structure.
\item Hoisting equivalent computations in non-sibling BBs.
\item Hoisting across switch blocks.
\item Ranking/Wavefront algorithm to hoist across switch blocks.
\end{itemize}


\section{Related Work}

A lot of bug reports in GCC and LLVM bugzillas, no papers on this topic yet.


graph algorithms \cite{tarjan},
SSA \cite{cytron}
GVN \cite{rosen1988global,click1995global}
CFG, DOM \cite{dragonbook}
SESE, SEME \cite{sese}
MemorySSA \cite{novillo2007memory}

\subsection{Global Value Numbering}

Code hoisting can also reduce the critical path length of execution in out of
order machines. As more instructios are available at the hoisting point, the
hardware has more instructions to reorder. Following example illustrates how
hoisting can improve performance by exposing more ILP.

\begin{verbatim}
float foo(float d, float min, float max, float a)
{
  float tmin, tmax, inv;

  inv = 1.0f / d;
  if (inv >= 0) {
    tmin = (min - a) * inv;
    tmax = (max - a) * inv;
  } else {
    tmin = (max - a) * inv;
    tmax = (min - a) * inv;
  }
  return tmax + tmin;
}
\end{verbatim}

In this program the computations of tmax and tmin are identical to the
computations of tmin and tmax of sibling branch respectively. Both tmax and tmin
depends on inv which depends on a division operation which is generally more
expensive than the addition, subtraction and multiplication operations. The
total latency of computation across each branch is:

O(div) + 2(O(sub) + O(mul))
Or for out of order processors with two add units and two multiply units:
O(div) + O(sub) + O(mul)

Now if the computation of tmax and tmin are hoisted outside the
conditionals, the C code version would look like this:
\begin{verbatim}
float foo(float d, float min, float max, float a)
{
  float tmin, tmax, tmin1, tmax1, inv;

  tmin1 = (min - a);
  tmax1 = (max - a);

  inv = 1.0f / d;
  tmin1 = tmin1 * inv;
  tmax1 = tmax1 * inv;

  if (inv >= 0) {
    tmin = tmin1;
    tmax = tmax1;
  } else {
    tmin = tmax1;
    tmax = tmin1;
  }

  return tmax + tmin;
}

\end{verbatim}

In this code the two subtractions and the division operations can be executed in
parallel because there are no dependencies among them. So the total number of
cycles will be max(O(div), O(sub)) + O(mul) = O(div) + O(mul); since O(div) is
usually much greater than O(sub) \cite{x86,aarch64}

Of course, a partial redundancy elimination pass could just remove the entire
if-block because final operation is an addition (asociative under fast math).

\begin{verbatim}
float foo(float d, float min, float max, float a)
{
  float tmin1, tmax1, inv;

  tmin1 = (min - a);
  tmax1 = (max - a);

  inv = 1.0f / d;
  tmin1 = tmin1 * inv;
  tmax1 = tmax1 * inv;

  return tmax1 + tmin1;
}
\end{verbatim}


\newpage

\section{Code hoisting}
For the code hoisting we follow a very systematic approach to writing a compiler
optimization pass. As such this pass can be divided into the following five
steps:

\subsection{Finding the candidates}
Finding if a set of instructions perform identical computations. Since this code
hoisting is based on GVN, we need the facility to compute GVN of instructions.
In llvm \cite{llvm}, GVN infrastructure is already set up and works well for
most kind of instructions except loads and stores so we compute the GVN of loads
and stores separately. In order to value number loads, we value number (hash)
the address from where the value is to be loaded. For stores, we value number
(hash) the address as well as the value to be stored at that address.

The process of computing GVN can be on-demand (as we come across an instruction)
or, precomputed (computing GVN of all the instructions beforehand). Which
process to choose is determined by the scope of code-hoisting we want to
perform. In a pessimistic approach (Section-\ref{subsec:pessimistic}) where we
want to hoist limited set of instructions from the sibing branches as we iterate
the DFS tree bottom-up, it is sufficient to compute values on-demand. Whereas,
in the optimistic approach (Section-\ref{subsec:optimistic}), where we want to
hoist as many instructions as possible, would require values to be precomputed.

Whatever approach we take, to find a candidate all we need to do is to compare
the value number of computations.

\subsection{Checking for legality}
Finding a hoisting point i.e., a common dominator of the set of instructions. If
such a dominator is found then checking if all the use-operands of the set of
instructions are available or not. In some cases when the operands are not
available, it is possible to reinstantiate (remateralize) the use-operands, thus
passing the legality check. Checking that the side-effects of the computations
does not intersect with any side-effects between the instructions to be hoisted
and their hoisting point. For memory operations like loads, stores, calls etc.,
it is also required to check that all the paths from the hoisting point to the
end of the function should execute the exact instruction, in order to guarantee
correctness. Moreover, hoisting of memory operations is tricky on paths which
have indirect branch targets e.g., landing pad, case statements, goto labels
etc., because it becomes difficult to prove that all the paths from hoisting
point to the end of the function would execute the instruction. In our current
implementation we discard hoisting through such paths.


In the optimistic approach (Section-\ref{subsec:optimistic}), it is possible
that a common hoisting point of all the instructions is either too far away, or
not legally possible. In these cases, it is still possible to `partially' hoist
a subset of instructions by splitting the set of candidates and finding a closer
hoisting point for each subset. For more details see
Section-\ref{subsec:partition}.

\subsubsection{Hoisting scalars}
Scalars are the easiest to hoist because we do not have to analyze them for
aliasing memory references. As long as all the operands are available, the
scalar can be hoisted.

\subsubsection{Hoisting loads}
The availability of operand to the load (an address) is checked at the hoisting
point. If that is not available we try to rematerialize the addresss if
possible.  Along the path, from current position of the load instruction through
the hoisting point, we find if there are aliasing writes to memory. In those
cases we discard the candidate.

\subsubsection{Hoisting stores}
We need to check the dependency requirements similar to the hoisting of
loads. We need to check that the operands of the store instruction are available
at the hoisting point, that there are no aliasing loads or store along the path
from the current position through the hoisting point.

\subsubsection{Hoisting calls}
For hoisting the call instructions, we check that the loads/stores in the call
would not hazard with any loads/stores (in the caller) along the path from the
callsite through the hoisting point, just like we did for loads and stores.

\subsection{Checking for profitability}
Establishing a set of cost models tune for performance. Like decreasing code
size, minimizing spills etc.

\subsubsection{Hoisting scalars}
Care must be taken in case of hoisting scalars too far, as that may increase
register pressure and result in spills. For example hoisting a scalar past a
call (Section-\ref{cost:across-calls}). In our current implementation we hoist
scalars past a call only when optimizing for code-side (-Os). Another way to
mitigate this problem is be to reinstantiate (rematerialize) the computation
after a call (may be as a different optimization pass).

\subsubsection{Hoisting loads}
A load instruction introduces a register where the value loaded will be kept,
the register pressure increases by one (unless the operand to load becomes dead
at the load). On the other hand, loading a value early will reduce the stall
during execution should the value is not in the cache. We generally prefer to
hoist load except the hoisting point is too far (this distance is computed by
looking at the experimental results of representative benchmarks see
Section-\ref{sec:experimental-results}).

\subsubsection{Hoisting stores}
Since stores do not increase the live-range of any registers, and in some cases
it ends the liveness of registers, we hoist all the stores.

\subsubsection{Hoisting calls}
Currently we hoist all the calls that are suitable candidates for hoisting.

\subsection{Making the transformation}
Once all the legality checks and profitability checks are satisfied for a set of
identical instructions, they are suitable candidates for hoisting. A copy of the
computation is inserted at the hoisting point along with any instructions which
needed to be rematerialized. Thereafter, all the computations made redundant by
the new copy are removed. After that SSA semantics is restored by updating the
intermediate representation (IR) to reflect changes.

\subsection{Verification}
Performing a set of checks (e.g., consistency of use-defs and SSA semantics) to
establish that program invariants are maintained.

\subsection{Hoisting bottom up with optimistic approach}
\label{subsec:optimistic}
Optimistic means collect the set of all potential redundancies and
then discard the bad ones.

Bottom up: Sort the basic blocks of potentially redundant computations
based on DFS-in numbers.  TODO: Explain why sorting by DFS-in numbers.
Then start evaluating the safety checks for first two, if good then
move to second one. That way we can even partially hoist computations
should we find that we cannot remove all the redundant computations.

We can also split the set of to partially hoist multiple times, if
hoisting globally is not possible.

In the case when more than one computation from the same basic block
shows up as redundant.

e.g.
\begin{verbatim}
BB1
load a;
....
store .;
....
load a;


BB2
load a

BB0 -> BB1
BB0 -> BB2
\end{verbatim}

In this case both the loads from BB1 will show up as redundant
(according to the current algorithm), but the complete hoising will
fail as the second load in BB1 is not safe to hoist. By following the
bottom-up approach, the sorted list of BBs will appear like this:

{ BB1, BB2(first load), BB2(second load) }

Now we will start evaluating the safety of BB1-l1 and BB2-l2 which
will pass, then BB12-l1 and BB2-l2 will fail.  So we can revert back
to BB12-l1 and hoist them to BB0. Doing the top down approach and
finding the partial set to hoist instructions partially would result
in a combinatorial explosion.

GCC: 23286 has interesting test cases.

\subsubsection{Partition the list of hoisting candidates to maximize hoisting}
\label{subsec:partition}
The set of candidates are sorted in the order of their DFS-in
numbers. So that candidates close to each other in the list are also
closer (??) in the CFG.

In the case where all the candidates cannot be hoisted at a common
hoist point because of:
\begin{enumerate}
\item Safety conditions are not met.
\item Live range would increase too much.
\end{enumerate}

In order to hoist a subset of identical instructions, we partition the
list of all candidates in a way to maximise the total number of
hoistings.  By sorting the list of all the candidates in the
increasing order of their DFS-in numbers, we make sure that candidates
closer in the list have their common dominator nearby.

Essentially,
abs(DFSIn(I1) - DFSIn(I2)) < abs(DFSIn(I1) - DFSIn(I3))
=> bbDist(I1, nearest-common-dom(I1, I2)) <= bbDist(I1, nearest-common-dom(I1, I3))

This means for candidates closer to each other w.r.t. DFS numbers:
\begin{enumerate}
\item would make fewer checks for legality and profitability.
\item hoisting point will be closer so, the intersection of live-range of the
  instruction with other instructions will be minimal.
\end{enumerate}

In our current implementation we partition the list at the point where the
legality/profitability checks fail.

TODO: Put an example to illustrate the point.

\subsection{Cost models}

\subsubsection{Reduce register pressure}
\label{hoist:reg-pressure}
Following example explains how code hoisting can actually reduce the register pressure.
Consider the following example where the labels prefixed with 'p' represent the position of
instruction in a basic block (names prefixed with bb).

\begin{verbatim}
bb0: p0: b = ...
bb1: p1: c = ...
bb2: p2: if (c) jump bb3 else jump bb4
bb3: p3: a0 = b + c
bb4: p4: a1 = b + c
bb5: p5: d = phi {a0(bb3), a1(bb4)}

If we measure distance(px, py) as total instruction
count in the path from the position of px to py

old-live-range = max(distance(p0, p3) + distance(p1, p3) ,
                     distance(p0, p4) + distance(p1, p4))
                 + distance (p3, p5)
new-live-rance = distance(p0, p2) + distance(p1, p2)
                 + distance (p2, p5)
\end{verbatim}

If the new live-range is less than the old one it will be a good candidate
for hoist. There are cases when both the ranges will be same:
\begin{enumerate}
  \item There is only one operand on the right hand side e.g., an assignment operation.
  \item One of the operands is not a Instruction/Register.
\end{enumerate}

In a special case where the instruction to be hoisted has the last use of its
operands then the code hoisting will always reduce the register pressure if it
has two register operands because the gain in live-range will be in the ratio of
2:1. Based on the above formulae we can also deduce that, as long as there is
one register operand in the right hand side with its last use, code hoisting
will either decrease or maintain the register pressure.

\subsubsection{In the presence of calls}
\label{cost:across-calls}
Hoisting scalars across calls is tricky because it can increase the number of
spills. During the frame lowering of calls, the argument registers, in general,
the caller saved registers are saved because they might be modified by the
callee and after the call they are restored \cite{frame-lowering}. So before the
call, the register pressure is high because the number of available registers
are reduced by the number of caller saved registers. In that situation if a
computation is hoisted across the call, that would increase the total number of
registers required by one, thus contributing to the register pressure.

However, in the two special cases discussed in Section-\ref{hoist:reg-pressure},
it will be okay to hoist because the regiter pressure would not change.

Hoisting memory references also require precise analysis of all the memory
addresses accessed by the call. In our implementation, since we only analyze the
callee declaration the analysis is very conservative. In the presence of pure
calls, loads can be hoisted but stores can't. Also, if the call throws exceptions,
or it it may not return, memory references cannot be hoisted.

\subsubsection{Hoisting too far away}
When we load a value from memory, we require an extra register to keep the value
around. Hoisting a load means we need to keep the value alive for a longer
period. That could result in spills later in the register allocation.

Also, if there are several instructions in between the hoisting point and the
instruction to be hoisted, the instruction to be hoisted crosses several
instructions while hoisting, it means we are adding one register to all the
live-ranges spanning the instructions. That could result in spills. In the
current implementation we choose to hoist if the number of instructions crossed
is below a threshold. Ideally, it should be okay to hoist all the instructions
and a later live-range-splitting pass should make the right decision of
rematerializing the instruction should it be beneficial to do so. But the
current live-range splitting pass of llvm is not making the optimial decision
and we have found spills if the threshold is exceeded. The threshold was
computed as a result of tuning the llvm testsuite \cite{llvm-nightly} and spec
benchmark \cite{Henning2000}.

\begin{enumerate}
\item When the hoist point is too far.
\item When the number of instructions we may cross is very high.
\end{enumerate}


\subsection{Other Approaches}

\subsubsection{Triangle approach}
This is the most basic approach to code-hoisting when the instructions are
hoisted to immediate parent in a diamond like structure. When there are
identical computations in both the branches of a conditional, and all the
legality checks are met, the instruction can be hoisted to the predecessor basic
block. This approach is simple to implement and has minimal impact on the
register pressure.

\subsubsection{Pessimistic approach}
\label{subsec:pessimistic}
To extend the triangle approach to the entire function, the algorithm traverses
the basic blocks in the invers depth-first order. All the leaf nodes are visited
before the non-leaf nodes because instructions are hoisted upwards. We start
from a branch and collect the GVN of all the instructions. Then in the sibling
basic block we find if there are any identical computations. Once we find an
identical computation we try to hoist it to the nearest common dominator which
is the immediate predecessor of both the basic blocks. We traverse the basic
block from top to down so that the once an instruction is hoisted it creates
opportunities for other dependent instructions. After all the sibling basic
blocks have all the identical computations hoisted, we continue the hoisting
process with their parent and its sibling of their parent until we reach the
beginning of the function.

\subsubsection{Optimistic approach}
In this approach, the goal is to maximise the total number of hoistings in the
entire function.  This algorithm is very useful when optimizing for code-size.
We see a lot of computations hoisted (see
Section~\ref{sec:experimental-results}. In order to do that, we collect the GVN
of all the instructions in the function. After that we iterate on the list of
instructions having identical GVN. After that we find the common dominator
dominating all the identical computations and try to hoist the instruction in
that basic block, if feasible. Many times it is not possible to hoist all the
instructions to one common dominator, due to legality/profitability
constraints. In those cases, the algorithmn would partion the list of
computations a partially hoist a set of computations to their common
dominator. For more details see Section-\ref{subsec:partition}.


\subsubsection{pros,cons}
Naturally, the amount of hoisting depends on the generality of the GVN
algorithm. Current llvm GVN implementation finds out equivalence between
dependent instructions to some extent.

\subsubsection{PRE based hoisting}

\newpage

\section{Experimental Evaluation}
\label{sec:experimental-results}

\subsection{Comparative analysis of different approaches to hoisting}


\section{Conclusion and Future Work}

\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
