\documentclass{sig-alternate}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{etoolbox}
\usepackage{flushend}
\usepackage{needspace}

\makeatletter
\preto{\@verbatim}{\topsep=1pt \partopsep=0pt}
\makeatother

\pagenumbering{arabic}

\begin{document}
\def \SCoP {SCoP}
\def \GCC {GCC}
\def \LLVM {LLVM}
\def \SESE {SESE}
\def \CFG {CFG}
\def \SSA {SSA}
\def \scev {scev}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\title{GVN-Hoist: Hoisting Computations from Branches}

\toappear{
   \hrule \vspace{5pt}
   LLVM-dev 2016
}
\numberofauthors{2}

\author{
\alignauthor
Aditya Kumar\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{aditya.k7@samsung.com}
\alignauthor
Sebastian Pop\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{s.pop@samsung.com}
\alignauthor
Daniel Berlin\\
       \affaddr{Google}\\
       \email{}
}

\maketitle
\begin{abstract}
  Code hoisting identifies identical computations across the program with the
  help of a Global Value Numbering (GVN) analysis, identifies a hoisting
  position that dominates all the identical computations and in which all
  operands are avaialable, duplicates one of the expressions in the hoisting
  position, and removes all the computations now redundant in their original
  position.

  Code hoisting is often confused with CSE (Common Subexpression Elimination)
  although hoisting is not bound to find redundancies within a single basic
  block.  Code hoisting could also be confused with Partial Redundancy
  Elimination (PRE) or with Global Common Subexpression Elimination (GCSE)
  although the main goal of code hoisting is not to remove redundancies: code
  hoisting effectively exposes redundancies and enables other passes like LICM
  to remove more redundancies.  The main goal of code hoisting is to reduce code
  size with the added benefit of exposing more instruction level parallelism and
  reduced register pressure.
\end{abstract}

\section{Introduction}

Compiler techniques to remove redundant computations are composed of an analysis
phase that detects identical computations in the program and a transformation
phase that reduces the number of run-time computations.  Classical scalar
optimizations like CSE \cite{dragonbook} work very well on single basic blocks.
When it comes to detect redundancies across basic blocks these techniques fall
short: more complex passes like GCSE and PRE have been designed to handle these
cases based on dataflow analysis \cite{morel1979global}.  At first these
techniques were described in the classical data-flow analysis framework, and
later the use of the SSA representation lowered the cost in terms of compilation
time \cite{briggs1994effective,chow1997new,kennedy1999partial} and brought these
techniques in the main stream: nowadays SSA based PRE is available in every
industrial compiler.

This paper describes code hoisting, a technique that uses the information
computed for PRE to detect identical computations and has a transformation phase
whose goal differs from PRE: it removes identical computations from different
branches of execution.  These identical computations would not be redundant at
run-time and as the number of run-time computations is not reduced, code
hoisting is not a redundancy elimination pass. This is also different from
global code scheduling \cite{dragonbook} in the sense that code-hoisting will
only hoist computations when there are two or more identical computation sharing
a common dominator. The goals of code hoisting are:

\begin{itemize}
\item to reduce the code size of the program;
\item to improve function inlining heuristics: functions become cheaper to
  inline by reducing their code size;
\item to expose more instruction level parallelism: by hoisting identical
  computations to be executed earlier, instruction schedulers can move heavy
  computations earlier in order to avoid pipeline bubbles;
\item to help out-of-order processors with speculative execution of branches: by
  hoisting expressions out of branches, code hoisting can effectively reduce the
  amount of code to be speculatively executed and can reduce the critical path;
\item to reduce register pressure: by moving computations closer to the
  definitions of their operands;
\item to improve passes that do not work well with branches:
  \begin{itemize}
  \item to improve loop vectorization by reducing a loop with control flow to a
    loop with a single BB, should all the instructions in a conditional get
    hoisted and sinked;
  \item to enable more loop invariant code motion (LICM): as LICM does not
    reason about instructions in the context of loops with conditional branches,
    code-hoisting is needed to move instructions out of conditional expressions
    and expose them to LICM.
  \end{itemize}
\end{itemize}

The main contributions of this paper are:
\begin{itemize}
\item a new algorithm to hoist computations from branches,
\item cost model to reduce live-range and reduce spills,
\item performance evaluation of the implementation in LLVM,
\item comparison against other algorithms for code hoisting.
\end{itemize}

\newpage

\section{Related Work}

Rosen et al. explain moving computations successors to remove redundancies
\cite{rosen1988global}. Their algorithm iterates on computations of same rank
and move the code with identical computations from the sibling branch. This is a
pessimistic approach (TODO: is it?). Also there is no notion of partially
hoisting the computations so their approach may result is missing many hoisting
opportunities.

A lot of bug reports in GCC and LLVM bugzillas, no papers on this topic yet.

Dhamdhere \cite{dhamdhere1988fast}, Muchnick \cite{steven1997advanced} mention
code hoisting in a data flow framework.


\newpage

\section{Code hoisting}

\subsection{}
The algorithm for code hoisting uses several common representations of the
program that we shortly describe below:
\begin{itemize}
\item Control Flow Graph (CFG) and the Dominance (DOM) and Post-Dominance (PDOM)
  relations \cite{dragonbook};
\item Single Entry Single Exit (SESE) \cite{sese} and Single Entry Multiple Exit
  (SEME) regions;
\item Static Single Assignment (SSA) \cite{cytron};
\item Global Value Numbering (GVN) \cite{rosen1988global,click1995global}: to
  identify similar computations compilers use GVN.  Each expression is given a
  unique number and the expressions that the compiler can prove identical are
  given the same number;
\item Memory SSA \cite{novillo2007memory}: memory operations that the compiler
  is able to prove in dependence are linked through use-def chains;
\end{itemize}


For the code hoisting we follow a very systematic approach to writing a compiler
optimization pass. As such this pass can be divided into the following five
steps:

\subsection{Finding the candidates}
The first step is to find if a set of instructions perform identical
computations. Since our code hoisting algorithm is based on GVN, we need the
facility to compute GVN of instructions.  In llvm \cite{llvm}, GVN
infrastructure is already set up and works well for most kind of instructions
except loads and stores so we compute the GVN of loads and stores separately. In
order to value number loads, we value number (hash) the address from where the
value is to be loaded. For stores, we value number (hash) the address as well as
the value to be stored at that address.

The process of computing GVN can be on-demand (as we come across an instruction)
or, precomputed (computing GVN of all the instructions beforehand). Which
process to choose is determined by the scope of code-hoisting we want to
perform. In a pessimistic approach (Section-\ref{subsec:pessimistic}) where we
want to hoist limited set of instructions from the sibing branches as we iterate
the DFS tree bottom-up, it is sufficient to compute values on-demand. Whereas,
in the optimistic approach (Section-\ref{subsec:optimistic}), where we want to
hoist as many instructions as possible, would require values to be precomputed.

Whatever approach we take, to find a candidate all we need to do is to compare
the value number of computations.

\subsection{Checking for legality}
\label{subsec:legality}
Since the equality of candidates is purely based on the value they compute, we
need to establish if hoisting them to a common dominator would be feasible. One
a common dominator is found then we check if all the use-operands of the set of
instructions are available or not. In some cases when the operands are not
available, it is possible to reinstantiate (remateralize) the use-operands, thus
passing the legality check. Subsequently, it is checked that the side-effects of
the computations does not intersect with any side-effects between the
instructions to be hoisted and their hoisting point. For memory operations like
loads, stores, calls etc., it is also required to check that all the paths from
the hoisting point to the end of the function should execute the exact
instruction, in order to guarantee correctness. Moreover, hoisting of memory
operations is tricky on paths which have indirect branch targets e.g., landing
pad, case statements, goto labels etc., because it becomes difficult to prove
that all the paths from hoisting point to the end of the function would execute
the instruction. In our current implementation we discard hoisting through such
paths.

In the optimistic approach (Section-\ref{subsec:optimistic}), it is possible
that a common hoisting point of all the instructions is either too far away, or
not legally possible. In these cases, it is still possible to `partially' hoist
a subset of instructions by splitting the set of candidates and finding a closer
hoisting point for each subset. For more details see
Section-\ref{subsec:partition}.

\subsubsection{Hoisting scalars}
Scalars are the easiest to hoist because we do not have to analyze them for
aliasing memory references. As long as all the operands are available (or can be
made available by rematerialization), the scalar can be hoisted.

\subsubsection{Hoisting loads}
The availability of operand to the load (an address) is checked at the hoisting
point. If that is not available we try to rematerialize the addresss if
possible.  Along the path, from current position of the load instruction through
the hoisting point, we find if there are aliasing writes to memory. In those
cases we discard the candidate.

\subsubsection{Hoisting stores}
We need to check the dependency requirements similar to the hoisting of
loads. We need to check that the operands of the store instruction are available
at the hoisting point, that there are no aliasing loads or store along the path
from the current position through the hoisting point.

\subsubsection{Hoisting calls}
For hoisting the call instructions, we check that the loads/stores in the call
would not hazard with any loads/stores (in the caller) along the path from the
callsite through the hoisting point, just like we did for loads and stores.

\subsection{Checking for profitability}
\label{subsec:profitability}
After the legality checks have passed, we check for profitability of hoisting.
That takes into account the impact code-hoisting would have on various
parameters that affects runtime performance e.g., impact on live-range, gain in
the code size.  We have established a set of cost models \ref{sec:cost-models}
for each parameter and tuned them for performance against representative
benchmarks.

\subsubsection{Hoisting scalars}
Since scalars are the majority of instructions which are hoisted, we pay special
attention in case of hoisting scalars too far, as that may increase register
pressure and result in spills. For example hoisting a scalar past a call
(Section-\ref{cost:across-calls}).  In our current implementation we hoist
scalars past a call only when optimizing for code-side (-Os). Ideally, a later
stage of live-range splitting pass should split the live-ranges for optimal
performance, however, that is not the case with llvm as we have found
regressions when scalars are hoisted too far \ref{sec:cost-models}. Another way
to mitigate this problem is be to reinstantiate (rematerialize) the computation
after a call (may be as a different optimization pass).

\subsubsection{Hoisting loads}
A load instruction introduces a register where the value loaded will be kept,
the register pressure increases by one (unless the operand to load becomes dead
at the load). On the other hand, loading a value early will reduce the stall
during execution should the value is not in the cache. We generally prefer to
hoist load except the hoisting point is too far (this distance is computed by
looking at the experimental results of representative benchmarks see
Section-\ref{sec:experimental-results}).

\subsubsection{Hoisting stores}
Since stores do not increase the live-range of any registers, and in some cases
it ends the liveness of registers, we hoist all the stores.

\subsubsection{Hoisting calls}
Currently we hoist all the calls that are suitable candidates for hoisting.

\subsection{Making the transformation}
Once all the legality checks and profitability checks are satisfied for a set of
identical instructions, they are suitable candidates for hoisting. A copy of the
computation is inserted at the hoisting point along with any instructions which
needed to be rematerialized. Thereafter, all the computations made redundant by
the new copy are removed. After that SSA form is restored by updating the
intermediate representation (IR) to reflect changes. After one iteration of
algorithm runs through the entire function, it creates more oppportunities for
\emph{higher ranked} computations \cite{rosen1988global}. Currently, we rerun
the algorithm until there are no more instructions left to be
hoisted. Obviously, this is not the most optimal approach and can be improved by
ranking the computations \cite{rosen1988global}.

\subsection{Verifying the post-conditions}
Performing a set of checks (e.g., consistency of use-defs and SSA semantics) to
establish that program invariants are maintained.


\subsection{Illustrative Example}
Code hoisting can also reduce the critical path length of execution in out of
order machines. As more instructios are available at the hoisting point, the
hardware has more instructions to reorder. Following example illustrates how
hoisting can improve performance by exposing more ILP.

\begin{verbatim}
float foo(float d, float min, float max, float a)
{
  float tmin, tmax, inv;

  inv = 1.0f / d;
  if (inv >= 0) {
    tmin = (min - a) * inv;
    tmax = (max - a) * inv;
  } else {
    tmin = (max - a) * inv;
    tmax = (min - a) * inv;
  }
  return tmax + tmin;
}
\end{verbatim}

In this program the computations of tmax and tmin are identical to the
computations of tmin and tmax of sibling branch respectively. Both tmax and tmin
depends on inv which depends on a division operation which is generally more
expensive than the addition, subtraction and multiplication operations. The
total latency of computation across each branch is:
$O(div) + 2(O(sub) + O(mul))$
Or, for out of order processors with two add units and two multiply units:
$O(div) + O(sub) + O(mul)$

Now if the computation of tmax and tmin are hoisted outside the
conditionals, the C code version would look like this:
\begin{verbatim}
float foo(float d, float min, float max, float a)
{
  float tmin, tmax, tmin1, tmax1, inv;

  tmin1 = (min - a);
  tmax1 = (max - a);

  inv = 1.0f / d;
  tmin1 = tmin1 * inv;
  tmax1 = tmax1 * inv;

  if (inv >= 0) {
    tmin = tmin1;
    tmax = tmax1;
  } else {
    tmin = tmax1;
    tmax = tmin1;
  }

  return tmax + tmin;
}

\end{verbatim}

In this code the two subtractions and the division operations can be executed in
parallel because there are no dependencies among them. So the total number of
cycles will be $max(O(div), O(sub)) + O(mul) = O(div) + O(mul)$; since $O(div)$ is
usually much greater than $O(sub)$ \cite{x86,aarch64}

Of course, a partial redundancy elimination pass could just remove the entire
if-block because final operation is an addition (asociative under fast math).

\begin{verbatim}
float foo(float d, float min, float max, float a)
{
  float tmin1, tmax1, inv;

  tmin1 = (min - a);
  tmax1 = (max - a);

  inv = 1.0f / d;
  tmin1 = tmin1 * inv;
  tmax1 = tmax1 * inv;

  return tmax1 + tmin1;
}
\end{verbatim}


GCC: 23286 has interesting test cases.

\section{Code hoisting policies}
The amount of hoisting depends on whether we collect GVN of instructions
before finding candidates (optimistic) or, on-demand (pessimistic). It also
depends on the generality of the GVN algorithm, however, that analysis is beyond
the scope of this paper.

\subsection{Hoisting bottom up with optimistic approach}
\label{subsec:optimistic}
In this approach, the goal is to maximise the total number of hoistings in the
entire function.  This algorithm is very useful when optimizing for code-size.
We collect the GVN of all the instructions in the function and iterate on the
list of instructions having identical GVNs. After that we find the common
dominator dominating all such identical computations and perform legality checks
(Section-\ref{subsec:legality}). Often times it is not possible to hoist all the
instructions to one common dominator, due to legality constraints e.g.,
intersecting side-effects or, profitability constraints e.g., hoisting point too
far. In those cases, this algorithmn would partition the list of identical
instructions into subsets which can be partially hoisted to their respective
common dominators. The partition algorithm is described as follows:

\subsubsection{Partition the list of hoisting candidates to maximize hoisting}
\label{subsec:partition}
In order to hoist a subset of identical instructions, we partition the list of
all candidates in a way to maximise the total number of hoistings.  By sorting
the list of all the candidates in the increasing order of their depth first
search discovery time stamp \cite{clrs} (DFSIn numbers), we make sure that
candidates closer in the list have their common dominator nearby in cases when
there are no fully redundant instructions. Essentially if,

\begin{verbatim}
// B1 != B2 != B3
BasicBlock B1, B2, B3
DFSIn(B1) = depth first discovery time stamp
DFSIn(B1,B2) = |DFSIn(B1) - DFSIn(B2)|
DFSIn(B1,B3) = |DFSIn(B1) - DFSIn(B3)|

Depth(B1) = depth of tree from the root node.
// When B1 dominates B2
BBDist(B1, B2) = |Depth(B1) - Depth(B2)|

NCD(B1, B2) = nearest common dominator of B1 and B2
BBDist(B1,B2) = BBDist(B1, NCD(B1, B2))
BBDist(B1,B3) = BBDist(B1, NCD(B1, B3))

Then,
DFSIn(B1,B2) < DFSIn(B1,B3) implies
 BBDist(B1,B2) <= BBDist(B1,B3)
\end{verbatim}

Same property would hold for instructions I1, I2, I3 provided they are not fully
redundant i.e., none of the basic blocks containing I1, I2 and I3 respectively
dominate the other:

\begin{verbatim}
Instruction I1, I2, I3
// I1, I2, I3 are not fully redundant
// B1, B2, B3 contains I1, I2, I3 respectively
DFSIn(I1) = DFSIn(B1)
BBDist(I1, I2) = BBDist(B1, B2)
DFSIn(I1) = DFSIn(B1)
DFSIn(I1,I3) = DFSIn(B1, B3)
NCD(I1, I2) = NCD(B1, B2)

Then,
 DFSIn(I1,I2) < DFSIn(I1,I3) implies
  BBDist(I1,I2) <= BBDist(I1,I3)
\end{verbatim}

It says that if I1 is closer to I2 than I3 in the list of candidates sorted by
DFSIn numbers, then the nearest common dominator of I1 and I2 will be closer (in
terms of basic block distance) to I1, than nearest common dominator of I1 and
I3.

In the presence of fully redundant computations in the list of hoistable
candidates this equation may not hold. For example, if there is another
instruction I4 which is dominated by I1, but I4 is present in a basic block
which is farther than DFSIn(I1, I2) i.e., $DFSIn(I1,I2) < DFSIn(I1,I4)$;
however, $BBDist(I1,I4) = 0 < BBDist(I1,I2) >=1$.

So by sorting candidates w.r.t. their DFSIn numbers:
\begin{enumerate}
\item would make fewer checks for legality and profitability.
\item hoisting point will be closer, so the intersection of live-range of the
  instruction with other instructions will be minimal.
\end{enumerate}

In our current implementation we keep as many candidates in one set as possible
(greedy approach). We split the list at a point where the legality checks fail
to hoist subset of candidates which are legal to hoist and then start finding
new hoisting point for the remaining ones. So there are two limitations of the
current implementation of the partition algorithm: First, sorting by DFSIn
numbers does not give the desired order when there are fully redundant
instructions; Second, it lacks cost model for partitioning in order to
preserve/improve performance \ref{sec:future-work}.

\subsection{Hoisting bottom up with pessimistic approach}
\label{subsec:pessimistic}
In the pessimistic approach, the basic blocks are traversed in the inverse
depth-first order, computing the GVN of instructions as they come by. The GVN of
sibling branches are compared for equality. Once such a candidate is found, it
is hoisted in the common dominator. All the leaf nodes are visited before the
non-leaf nodes (bottom-up) because instructions are hoisted upwards.

The pessimistic algorithm is fast, results in fewer spills but hoists very less
instructions. We have implemented optimistic approach because it is more general
and can be tuned down to closely mimic pessimistic approach by changing a flag.

\subsection{Time complexity of algorithm}
The complexity of code hoisting is linear in number of instructions that could
be hoisted in the program, matching the complexity of PRE on SSA form.  The
analysis phase is based on the Global Value Numbering (GVN), the same analysis
used for PRE, followed by the computation of a partition of identical
expressions to be hoisted in a same location to guarantee safety properties and
program performance, and followed by a simple code generation that adds the
identified instruction in the hoisting point and removes all the now redundant
expressions.

\section{Cost models}
\label{sec:cost-models}
Similar to any compiler optimization pass, there are several cost functions that
are deployed to tune for optimal combination of performance and code-size.
Since this is mostly a code-size optimization pass, the goal is to not regress
in performance across popular benchmarks at the same time reduce code size as
much as possible. Following are the cost models which are implemented:

\subsection{Reduce register pressure}
\label{hoist:reg-pressure}
Following example explains how code hoisting can actually reduce the register
pressure.  Consider the following example where the labels prefixed with 'P'
represent the position of instruction in a basic block (names prefixed with 'B').

\begin{verbatim}
B0: P0: b = 1
    goto B1

B1: P1: c = 2
    goto B2

B2: P2: if c is true then goto B3 else goto B4

B3: P3: a0 = b + c
    goto B5

B4: P4: a1 = b + c
    goto B5

B5: P5: d = phi {a0(B3), a1(B4)}

If we measure D(Px,Py) as total instruction
count in the path from the position of Px to Py

live-range(a0) = D(P0,P3) + D(P1,P3) + D(P3,P5)
               = 6 + 4 + 2 = 12
live-range(a1) = D(P0,P4) + D(P1,P4) + D(P4,P5)
               = 6 + 4 + 2 = 12

old-live-range = max(live-range(a0),live-range(a1))
               = 12

After hoisting a0 and a1 are removed and a copy
of a0 as a01 is placed in B2 just before P2.

live-range(a01) = D(P0,P2-1) + D(P1,P2-1) + D(P2 -1,P5)
                = 4 + 3 + 3 = 10
new-live-range = live-range(a01)
\end{verbatim}

If the new live-range is less than the old one it will be a good candidate for
hoisting. The live ranges can remain same as well if there is only one operand
on the right hand side e.g., an assignment operation or, one of the operands is
not a register. That means hoisting upwards will decrease the live-range of its
use but increase the live-range of its definition.

In a special case where the instruction to be hoisted has the last use of its
operands then the code hoisting will always reduce the register pressure if it
has two register operands because the gain in live-range will be in the ratio of
2:1. Based on the above formulae we can also deduce that, as long as there is
one register operand in the right hand side with its last use, code hoisting
will either decrease or preserve the register pressure.

\subsection{In the presence of calls}
\label{cost:across-calls}
Hoisting scalars across calls is tricky because it can increase the number of
spills. During the frame lowering of calls, the argument registers, in general,
the caller saved registers are saved because they might be modified by the
callee and after the call they are restored \cite{frame-lowering}. So before the
call, the register pressure is high because the number of available registers
are reduced by the number of caller saved registers. In that situation if a
computation is hoisted across the call, that would increase the total number of
registers required by one, thus contributing to the register pressure. However,
in the special case discussed in Section-\ref{hoist:reg-pressure}, it will be
okay to hoist because the register pressure would not decrease.

Hoisting loads/stores across calls also require precise analysis of all the
memory addresses accessed by the call. Our implementation being an
intraprocedural pass, the analysis is very conservative. In the presence of pure
calls, loads can be hoisted but stores can't. Also, if the call throws
exceptions, or it it may not return, memory references cannot be hoisted.

\subsection{Hoisting too far away}
If there are several instructions in between the hoisting point and the
instruction to be hoisted, the instruction to be hoisted crosses several
instructions while hoisting, that means we are adding one register to all the
live-ranges spanning the instructions. That could result in spills. In the
current implementation we choose to hoist if the number of instructions crossed
is below a threshold. Ideally, it should be okay to hoist all the instructions
and a later a live-range-splitting \cite{cooper1998live} pass should make the
right decision of rematerializing the instruction should it be beneficial to do
so. But the current live-range splitting pass of llvm is not making the optimial
decision and we have found spills if the threshold is exceeded. The threshold
was computed as a result of tuning the llvm testsuite \cite{llvm-nightly} and
spec benchmark \cite{Henning2000}.

Also, hoisting a load increases the register pressure by one across all the
instructions which the load would cross. That could result in spills later in
the register allocation.

However, in the special case discussed in Section-\ref{hoist:reg-pressure}, it
will be okay to hoist because the register pressure would not decrease.


\section{Experimental Evaluation}
\label{sec:experimental-results}
We ran llvm-testsuite (trunk:d87471f8) with the patch (trunk:86940146) and the
results are listed in Table-\ref{tab:hoist-results}. The table lists the number
of scalars, loads, stores and calls hoisted as well as removed. For each
category, the number of instructions removed is greater or equal to the number
of instructions hoisted because each hoisting is performed only when at least
one identical computation is found.

Loads are hoisted the most followed by scalars, stores and calls in decreasing
order.  This was the common trend in all our experiments. One reason why loads
are hoisted the most is the early execution of this pass (before mem2reg) in the
llvm pass pipeline. Passes like mem2reg, instcombine might actually remove
those loads so this order may change should this pass be scheduled later.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|}
      \hline
      Metric               & Number\\\hline
      Scalars hoisted      & 6791  \\\hline
      Scalars removed      & 9696  \\\hline
      Loads hoisted        & 14802 \\\hline
      Loads removed        & 20719 \\\hline
      Stores hoisted       & 15    \\\hline
      Stores removed       & 15    \\\hline
      Calls hoisted        & 8     \\\hline
      Calls removed        & 8     \\\hline
      Total Instructions hoisted & 21616 \\\hline
      Total Instructions removed & 30438 \\\hline
\end{tabular}
  \end{center}
  \caption{Code hoisting metrics on llvm-testsuite}
  \label{tab:hoist-results}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|c|}
      \hline
      Metric               & Before & After              \\\hline
      Call sites deleted, not inlined             & 1988    & 1988   \\\hline
      Functions deleted (all callers found)       & 38250   & 38255  \\\hline
      Functions inlined                           & 154986  & 154985 \\\hline
      Allocas merged together                     & 212     & 212    \\\hline
      Caller-callers analyzed                     & 193042  & 193092 \\\hline
      Call sites analyzed                         & 414336  & 414381 \\\hline
      Rematerialized defs for spilling            & 18321   & 18326  \\\hline
      Rematerialized defs for splitting           & 5719    & 5842   \\\hline
      Spill slots allocated                       & 42912   & 42970  \\\hline
      Spilled live ranges                         & 61330   & 61362  \\\hline
      Spills inserted                             & 50724   & 50784  \\\hline
\end{tabular}
  \end{center}
  \caption{Static metrics before and after code-hoisting on llvm-testsuite}
  \label{tab:static-results}
\end{table}


Other static metrics are listed in Table-\ref{tab:static-results}. Here we can
see that except for rematerializing defs for splitting, which has an overhead of ~2\%, all
other parameters have less than 1\% overhead. This is to explain why the performance does not
go down with our implementation (and cost-model) of code hoisting pass.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|c|}
      \hline
      Code-size metric  (.text)                   & Number   \\\hline
      Total  benchmarks                           & 497      \\\hline
      Total  gained in size                       & 39       \\\hline
      Total  decrease in size                     & 58       \\\hline
      Median decrease in size                     & 2.9\%    \\\hline
      Median increase in size                     & 2.4\%    \\\hline
    \end{tabular}
  \end{center}
  \caption{Code size metrics on llvm-testsuite}
  \label{tab:code-size}
\end{table}

While benchmarking llvm-testsuite we see both increase as well as decrease in
the codesizes of the final binaries. Since the pass runs early, it has affect on
many optimizations which rely on number of instructions, lenth of the use-def
chain etc metrics which are affected by code-hoisting. Specially the inliner
where the total inline cost (in llvm) is heavily dependent on the total count of
instructions in the caller and callee. Various code-size metrics are shown in
Table-\ref{tab:code-size}. All but one benchmark varied between -5.32\% and
5.43\%.  In one benchmark FreeBench/distray/distray.test, the codesize increased
by 35.38\%. In this benchmark 3 more (15 as compared to 12) functions got
inlined and because of that 10 more (81 vs. 71) vector instructions got
generated, 3 calls got hoisted/sunk as (compared to 0), one loop got unswitched
(compared to 0), 6 high latency machine instructions got hoisted out of loop, 59
(compared ot 30) machine instructions got hoisted out of loop, 70 (compared to
39) machine instructions were sunk.

\section{Conclusion and Future Work}
\label{sec:future-work}
We have presented the GVN based code hoisting algorithm. The primary goal is to
reduce the code size but it benefits performance in some cases as well. To
preserve performance and not hoist too much we have implemented several cost
models \ref{sec:cost-models}. Since those cost models depend on a set of thresholds,
it requires tuning, as such, we used representative benchmarks to tune them.

Currently, we rerun the algorithm until there are no more instructions left to
be hoisted. This is not the most optimal approach and results in expensive
analyses to be recomputed. This can be improved by ranking the computations
\cite{rosen1988global}.

With the implementation of code-hoisting in llvm, the passes which rely on the
code-size/instruction-count to make optimization decisions needs to be
revisited. The first candidate would be the inliner. We have seen different
inlining decisions (See Table-\ref{tab:code-size}) before and after
code-hoisting was enabled.  Since inliner has several magic numbers tuned for
the previous pass layout, it would need some improvement.


\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
