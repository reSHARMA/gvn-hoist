\section{Introduction}

\section{Global Value Numbering}
\section{Code Hoisting}
Code hoisting does not remove redundancies by itself but it enables
removal of redundancies by other passes.  e.g., Enable inlining.
\section{Related Work}

\begin{section}

1. Code hoisting in adjacent blocks in a diamond/triangle like structure.
2. O(n) Algorithm.
3. Hoisting equivalent computations in non-sibling BBs.
4. Hoisting across switch blocks.
5. Hoisting to improve PRE.
6. Hoisting to reduce critical path in OoO execution.
7. Ranking/Wavefront algorithm to hoist across switch blocks.


\section{Hoisting to improve performance}
Hoisting also reduces critical path length of execution in out of
order machines (but not in sequential machines), by exposing ILP
before the conditional where the instruction was hoisted.

\begin{program}
float foo(float d, float min, float max, float a)
{
  float tmin;
  float tmax;

  float inv = 1.0f / d;
  if (inv >= 0) {
    tmin = (min - a) * inv;
    tmax = (max - a) * inv;
  } else {
    tmin = (max - a) * inv;
    tmax = (min - a) * inv;
  }
  return tmax + tmin;
}

\end{program}

In this program the computations of tmax and tmin and identical in
both the branches but they cannot be evaluated before inv. Since inv
depends on a division operation which is generally more expensive than
the addition, subtraction and multiplication operations as required to
evaluate tmax and tmin, the total latency of computation across each
branch is:

O(div) + 2(O(sub) + O(mul))
Or for out of order processors with two add units and two multiply units:
O(div) + O(sub) + O(mul)

But if we hoist the computation of tmax and tmin outside the
conditionals, the C code version would look like this:
\begin{program}
float foo(float d, float min, float max, float a)
{
  float tmin;
  float tmax;

  tmin' = (min - a);
  tmax' = (max - a);

  float inv = 1.0f / d;
  tmin' = tmin' * inv;
  tmax' = tmax' * inv;

  if (inv >= 0) {
    tmin = tmin';
    tmax = tmax';
  } else {
    tmin = tmax';
    tmax = tmin';
  }

  return tmax + tmin;
}

\end{program}

In this code the two subtractions and the div operations can execute
in parallel. So the total number of cycles will be O(div) + O(mul)
since O(div) is usually greater than O(sub)

Of course, a sinking optimization could just sink the computation of
both tmax and tmin, and since the final operation is an addition
(asociative under fast math), the entire if-else code would go away.

\begin{program}
float foo(float d, float min, float max, float a)
{
  float tmin;
  float tmax;

  tmin' = (min - a);
  tmax' = (max - a);

  float inv = 1.0f / d;
  tmin' = tmin' * inv;
  tmax' = tmax' * inv;

  return tmax' + tmin';
}
\end{program}

\section{Hoisting scalars}
Scalars are the easiest to hoist because we do not have to analyze
them for aliasing memory references. As long as all the operands are
available (which in SSA form we do, or may be there are chained
dependencies), the scalar can be hoisted. Care must be taken in case
of hoisting scalars too far, as that may increase register pressure
and result in spills. For example hoisting a scalar past a call. In
that case the call may result in save and restore of the register the
scalar may be defined to. In our current implementation we do not
hoist scalars past a call. Another way to mitigate this problem is be
to reinstantiate (rematerialize) the computation after a call (may be
as a different pass).



\section{Using MemorySSA}
\subsection{Iterative update to memory ssa during the hoisting process}

\section{Without MemorySSA using the dominance information}

\section{Hoisting stores}
We need to value number the address and the value being stored.

\section{Hoisting calls}
Calls can be hoisted by value-numbering all the arguments. For safety
we have to check that there are no other side-effects between a call and
its hoisting point, just like loads and stores.


\section{Comparative analysis of different approaches to hoisting}
\subsection{Optimistic approach}
Collect GVN of all the instructions in the beginning.

\subsection{Triangle approach}
This is the most basic approach to code-hoisting when the instructions are
hoisted to immediate parent in a diamond like structure. When there are
identical computations in both the branches of a conditional, and all the
legality checks are met, the instruction can be hoisted to the predecessor basic
block. This approach is simple to implement and has minimal impact on the
register pressure.

\subsection{Pessimistic approach}
To extend the triangle approach to the entire function, the algorithm traverses
the basic blocks in the invers depth-first order. All the leaf nodes are visited
before the non-leaf nodes because instructions are hoisted upwards. We start
from a branch and collect the GVN of all the instructions. Then in the sibling
basic block we find if there are any identical computations. Once we find an
identical computation we try to hoist it to the nearest common dominator which
is the immediate predecessor of both the basic blocks. We traverse the basic
block from top to down so that the once an instruction is hoisted it creates
opportunities for other dependent instructions. After all the sibling basic
blocks have all the identical computations hoisted, we continue the hoisting
process with their parent and its sibling of their parent until we reach the
beginning of the function.

\section{Hoisting bottom up with optimistic approach}
Optimistic means collect the set of all potential redundancies and
then discard the bad ones.

Bottom up: Sort the basic blocks of potentially redundant computations
based on DFS-in numbers.  TODO: Explain why sorting by DFS-in numbers.
Then start evaluating the safety checks for first two, if good then
move to second one. That way we can even partially hoist computations
should we find that we cannot remove all the redundant computations.

We can also split the set of to partially hoist multiple times, if
hoisting globally is not possible.

In the case when more than one computation from the same basic block
shows up as redundant.

e.g.
BB1
load a;
....
store .;
....
load a;


BB2
load a

BB0 -> BB1
BB0 -> BB2

In this case both the loads from BB1 will show up as redundant
(according to the current algorithm), but the complete hoising will
fail as the second load in BB1 is not safe to hoist. By following the
bottom-up approach, the sorted list of BBs will appear like this:

{ BB1, BB2(first load), BB2(second load) }

Now we will start evaluating the safety of BB1-l1 and BB2-l2 which
will pass, then BB12-l1 and BB2-l2 will fail.  So we can revert back
to BB12-l1 and hoist them to BB0. Doing the top down approach and
finding the partial set to hoist instructions partially would result
in a combinatorial explosion.

GCC: 23286 has interesting test cases.

\section{Partition the list of hoisting candidates to maximize hoisting}
The set of candidates are sorted in the order of their DFS-in
numbers. So that candidates close to each other in the list are also
closer (??) in the CFG.

In the case where all the candidates cannot be hoisted at a common
hoist point because of:
\begin{enumerate}
\item Safety conditions are not met.
\item Live range would increase too much.
\end{enumerate}

In order to hoist a subset of identical instructions, we partition the
list of all candidates in a way to maximise the total number of
hoistings.  By sorting the list of all the candidates in the
increasing order of their DFS-in numbers, we make sure that candidates
closer in the list have their common dominator nearby.

Essentially,
abs(DFSIn(I1) - DFSIn(I2)) < abs(DFSIn(I1) - DFSIn(I3))
=> bbDist(I1, nearest-common-dom(I1, I2)) <= bbDist(I1, nearest-common-dom(I1, I3))

This means for candidates closer to each other w.r.t. DFS numbers:
\begin{enumerate}
\item would make fewer checks for preconditions.
\item hoisting point will be closer so, the intersection of live-range of the instruction
with other instructions will be minimal.
\end{enumerate}

In our current implementation we partition the list at the point where the
legality/profitability checks fail.

TODO: Put an example to illustrate the point.

\section{Cost models}
\subsection{In the presence of calls}
Hoisting scalars across calls is tricky because it can increase the
number of spills.

\subsection{Hoisting too far away}
\begin{enumerate}
\item When the hoist point is too far.
\item When the number of instructions we may cross is very high.
\end{enumerate}



\section{Enabling LICM by code-hoisting}
loop:
if (a)
  i1;
else
  i1;

Here i1 is redundant and can be hoisted out of loop. But the LICM will
not do because it does not reason about instructions in the
conditional.  By code-hoisting i1 will be hoisted out of conditional
which will benefit the LICM.

\section{Improving the Inliner heuristics}
Reducing the number of instructions also reduces the inlining cost and
hence improves the inliner heuristics.

\section{Improving vectorization}
It will also benefit vectorizer by reducing the number
of basic blocks, should all the instructions in conditional get hoisted.

\end{section}


